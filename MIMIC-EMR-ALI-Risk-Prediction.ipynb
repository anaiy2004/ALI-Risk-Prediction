{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of MIMICEMREventPrediction(Working).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPMr9NCbUX/+stMfh1Yo3Wb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anaiy2004/ALI-Risk-Prediction/blob/main/MIMIC-EMR-ALI-Risk-Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWyIKqlKBtlo"
      },
      "source": [
        "!wget -r -N -c -np --user anaiysomalwar --ask-password https://physionet.org/files/mimiciii/1.4/    \n",
        "#reading in files from MIMIC III (password protected)\n",
        "#run this directly on terminal when using gcp\n",
        "#output of code cell deleted for privacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-7g0Q2hS9_S",
        "outputId": "3ad5af92-7c6d-40b2-c85b-a3a9d0c2758d"
      },
      "source": [
        "#allows for Pyspark / SQL use on Google Collab\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://apache.claz.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "!pip3 install findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.7/dist-packages (1.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVxgcrZgWrQ_"
      },
      "source": [
        "#https://github.com/dhlee4/AcuteOrganFailureInterventionModel/blob/master/mimic_hp_training_scale.py lines 117 - end of file -> first step of recreation calls mimic_run_experiment on line 131\n",
        "#since mimic_run_experiment is called, we move to line 25 in that same file, where the class has the constructor\n",
        "#on line 31, mimic_preprocesser is called is constructed, so we go to https://github.com/dhlee4/AcuteOrganFailureInterventionModel/blob/master/mimic_preprocess.py\n",
        "#on line 8, mimic_data_abstracter is called so we go to https://github.com/dhlee4/AcuteOrganFailureInterventionModel/blob/master/mimic_data_abstracter.py -> does nothing but def methods\n",
        "#on line 9 from mimic_preprocess, data_preprocessor is initialized https://github.com/dhlee4/AcuteOrganFailureInterventionModel/blob/master/data_preprocessor.py\n",
        "#where data_preprocessor is initialized, preprocessor_gen is called\n",
        "\n",
        "#Target Disease: 42731, 5849, 51881,5990\n",
        "from pyspark.sql.functions import col,when,lit\n",
        "# this target disease does not do anyything\n",
        "target_disease=[\"51881\"]\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEAjr5vxWnJX"
      },
      "source": [
        "def add_demo():\n",
        "        import pyspark\n",
        "        from pyspark.sql.functions import datediff,col\n",
        "        from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
        "        from pyspark.ml.feature import VectorAssembler\n",
        "        import pandas as pd\n",
        "\n",
        "        cur_demo = pd.read_csv(\"physionet.org/files/mimiciii/1.4/PATIENTS.csv.gz\", low_memory= False)\n",
        "        patientsFinalColumns = [\"SUBJECT_ID\", \"DOB\", \"GENDER\"]\n",
        "        cur_demo = cur_demo[cur_demo.columns.intersection(patientsFinalColumns)]\n",
        "        \n",
        "        cur_pts = pd.read_csv(\"physionet.org/files/mimiciii/1.4/ADMISSIONS.csv.gz\",  low_memory= False)\n",
        "        admissionsFinalColumnNames = [\"SUBJECT_ID\", \"HADM_ID\", \"ADMITTIME\", \"ADMISSION_TYPE\", \"ADMISSION_LOCATION\", \"INSURANCE\", \"LANGUAGE\", \"RELIGION\", \"MARITAL_STATUS\", \"ETHNICITY\"]\n",
        "        cur_pts = cur_pts[cur_pts.columns.intersection(admissionsFinalColumnNames)]\n",
        "\n",
        "        from pyspark.context import SparkContext\n",
        "        from pyspark.sql.session import SparkSession\n",
        "        sc = SparkContext.getOrCreate();\n",
        "        spark = SparkSession(sc)\n",
        "        cur_pts.LANGUAGE = cur_pts.LANGUAGE.astype(str)\n",
        "        cur_pts.MARITAL_STATUS = cur_pts.MARITAL_STATUS.astype(str)\n",
        "        cur_pts.RELIGION = cur_pts.RELIGION.astype(str)\n",
        "        cur_demo = spark.createDataFrame(cur_demo)\n",
        "        cur_pts = spark.createDataFrame(cur_pts)\n",
        "        merged_demo = cur_demo.join(cur_pts,\"SUBJECT_ID\").drop(\"SUBJECT_ID\")\n",
        "        merged_demo = merged_demo.withColumn(\"AGE\",datediff(\"ADMITTIME\",\"DOB\")/365.0).withColumn(\"AGE\",when(col(\"AGE\")>90,90).otherwise(col(\"AGE\"))).drop(\"ADMITTIME\",\"DOB\").where(\"AGE > 18\").fillna(\"N/A\")\n",
        "\n",
        "        target_col = merged_demo.columns\n",
        "        target_col.remove(\"AGE\")\n",
        "        target_col.remove(\"HADM_ID\")\n",
        "        target_col.sort()\n",
        "        vector_target = [\"AGE\"]\n",
        "        demo_col_list = [\"AGE\"]\n",
        "        for cat_col in target_col:\n",
        "            SI_model= StringIndexer(inputCol=cat_col, outputCol=\"SI_{0}\".format(cat_col)).fit(merged_demo)\n",
        "            demo_col_list = demo_col_list+[demo_var+\"||\"+demo_info for demo_var, demo_info in (zip([cat_col]*len(SI_model.labels),SI_model.labels))]\n",
        "            merged_demo = SI_model.transform(merged_demo)\n",
        "            merged_demo = OneHotEncoder(inputCol=\"SI_{0}\".format(cat_col),outputCol=\"OH_{0}\".format(cat_col), dropLast=False).fit(merged_demo).transform(merged_demo)\n",
        "            vector_target.append(\"OH_{0}\".format(cat_col))\n",
        "\n",
        "        import json\n",
        "\n",
        "        return_df = VectorAssembler(inputCols=vector_target,outputCol=\"demo_feature\").transform(merged_demo)\n",
        "        return return_df.withColumnRenamed(\"HADM_ID\", \"ID\").select(\"ID\",\"demo_feature\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TKIjA8_N_YU"
      },
      "source": [
        "#x = add_demo()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOwslu8CQyVJ"
      },
      "source": [
        "#look at https://github.com/dhlee4/AcuteOrganFailureInterventionModel/blob/master/mimic_data_abstracter.py\n",
        "####################### -> Part 2 of the Code -> equivalent to get_df_df method\n",
        "#https://github.com/dhlee4/AcuteOrganFailureInterventionModel/blob/master/mimic_data_abstracter.py lines 22 -> 39\n",
        "def get_def_df():\n",
        "  import pandas as pd\n",
        "  cur_chart_df = pd.read_csv(\"physionet.org/files/mimiciii/1.4/D_ITEMS.csv.gz\")\n",
        "  cur_chart_df = cur_chart_df[cur_chart_df[\"LINKSTO\"] == \"chartevents\"]\n",
        "  cur_chart_dfFinalColumns = [\"ITEMID\",\"LABEL\"]\n",
        "  cur_chart_df = cur_chart_df[cur_chart_df.columns.intersection(cur_chart_dfFinalColumns)]\n",
        "  cur_chart_df_AddedList = []\n",
        "  for i in range(len(cur_chart_df)):\n",
        "    cur_chart_df_AddedList.append( \"VITAL\") #this is the .alias lit() segment in SQL\n",
        "  cur_chart_df[\"SOURCE\"] = cur_chart_df_AddedList\n",
        "  cur_chart_df.head()\n",
        "\n",
        "  cur_med_df = pd.read_csv(\"physionet.org/files/mimiciii/1.4/D_ITEMS.csv.gz\")\n",
        "  cur_med_df = cur_med_df[cur_med_df[\"LINKSTO\"] == \"inputevents_mv\"]\n",
        "  cur_med_dfFinalColumns = [\"ITEMID\", \"LABEL\"]\n",
        "  cur_med_df = cur_med_df[cur_med_df.columns.intersection(cur_med_dfFinalColumns)]\n",
        "  cur_med_df_AddedList = []\n",
        "  for i in range(len(cur_med_df)):\n",
        "    cur_med_df_AddedList.append(\"MED\") #this is the .alias lit() segment in SQL\n",
        "  cur_med_df[\"SOURCE\"] = cur_med_df_AddedList\n",
        "  cur_med_df.head()\n",
        "\n",
        "  cur_proc_df = pd.read_csv(\"physionet.org/files/mimiciii/1.4/D_ITEMS.csv.gz\")\n",
        "  cur_proc_df = cur_proc_df[cur_proc_df[\"LINKSTO\"] == \"procedureevents_mv\"]\n",
        "  cur_proc_dfFinalColumns = [\"ITEMID\", \"LABEL\"]\n",
        "  cur_proc_df = cur_proc_df[cur_proc_df.columns.intersection(cur_proc_dfFinalColumns)]\n",
        "  cur_proc_df_AddedList = []\n",
        "  for i in range(len(cur_proc_df)):\n",
        "    cur_proc_df_AddedList.append(\"PROC\") #this is the .alias lit() segment in SQL\n",
        "  cur_proc_df[\"SOURCE\"] = cur_proc_df_AddedList\n",
        "  cur_proc_df.head()\n",
        "\n",
        "  cur_lab_df = pd.read_csv(\"physionet.org/files/mimiciii/1.4/D_ITEMS.csv.gz\")\n",
        "  cur_lab_dfFinalColumns = [\"ITEMID\", \"LABEL\"]\n",
        "  cur_lab_df = cur_lab_df[cur_lab_df.columns.intersection(cur_lab_dfFinalColumns)]\n",
        "  cur_lab_df_AddedList = []\n",
        "  for i in range(len(cur_lab_df)):\n",
        "    cur_lab_df_AddedList.append(\"LAB\") #this is the .alias lit() segment in SQL\n",
        "  cur_lab_df[\"SOURCE\"] = cur_lab_df_AddedList\n",
        "  cur_lab_df.head()\n",
        "\n",
        "  #double check whether \\ in SQL is just a line break\n",
        "  import pandas as pd\n",
        "  #what does lit($$) mean? I am just adding together things instead of the .concat method, but there is lit($$) between them, which I am assuming is \" \"\n",
        "  cur_cpt_df = pd.read_csv(\"physionet.org/files/mimiciii/1.4/CPTEVENTS.csv.gz\").fillna(\"\")\n",
        "  cur_cpt_df_renamedCPT_CD = cur_cpt_df[\"CPT_CD\"]\n",
        "  cur_cpt_df[\"ITEMID\"] = cur_cpt_df_renamedCPT_CD\n",
        "  cur_cpt_df_concatdescription = []\n",
        "  for i in range(len(cur_cpt_df)):\n",
        "    cur_cpt_df_concatdescription.append(cur_cpt_df[\"SECTIONHEADER\"][i] + \" \" + cur_cpt_df[\"SUBSECTIONHEADER\"][i] + cur_cpt_df[\"DESCRIPTION\"][i]) #double check this\n",
        "  cur_cpt_df[\"LABEL\"] = cur_cpt_df_concatdescription\n",
        "  cur_cpt_df_AddedList = []\n",
        "  for i in range(len(cur_cpt_df)):\n",
        "    cur_cpt_df_AddedList.append(\"CPT\") #this is the .alias lit() segment in SQL\n",
        "  cur_cpt_df[\"SOURCE\"] = cur_cpt_df_AddedList\n",
        "  cur_cpt_dfFinalColumns = [\"ITEMID\", \"LABEL\", \"SOURCE\"]\n",
        "  cur_cpt_df = cur_cpt_df[cur_cpt_df.columns.intersection(cur_cpt_dfFinalColumns)]\n",
        "  cur_cpt_df = cur_cpt_df.drop_duplicates(inplace = False)\n",
        "  cur_cpt_df.head()\n",
        "  # currently on line 32 of github\n",
        "\n",
        "  cur_icd_df = pd.read_csv(\"physionet.org/files/mimiciii/1.4/D_ICD_DIAGNOSES.csv.gz\")\n",
        "  cur_icd_df_renamedICD9Code = cur_icd_df[\"ICD9_CODE\"]\n",
        "  cur_icd_df[\"ITEMID\"] = cur_icd_df_renamedICD9Code\n",
        "  cur_icd_df_renamedLongTitle = cur_icd_df[\"LONG_TITLE\"]\n",
        "  cur_icd_df[\"LABEL\"] = cur_icd_df_renamedLongTitle\n",
        "  cur_icd_df_AddedList = []\n",
        "  for i in range(len(cur_icd_df)):\n",
        "    cur_icd_df_AddedList.append(\"ICD_DIAGNOSIS\") #this is the .alias lit() segment in SQL\n",
        "  cur_icd_df[\"SOURCE\"] = cur_icd_df_AddedList\n",
        "  cur_icd_dfFinalColumns = [\"ITEMID\", \"LABEL\", \"SOURCE\"]\n",
        "  cur_icd_df = cur_icd_df[cur_icd_df.columns.intersection(cur_icd_dfFinalColumns)]\n",
        "  cur_icd_df.head()\n",
        "\n",
        "  #what are we merging the dataframes by? -> assuming that we just stack them and remove duplicates, which is what the union operator seems to be\n",
        "  def_df = pd.concat([cur_med_df, cur_proc_df, cur_chart_df, cur_lab_df, cur_cpt_df,cur_icd_df]).drop_duplicates()\n",
        "  print(len(def_df))\n",
        "  def_df.head()\n",
        "  return def_df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa4klIQZOpjt"
      },
      "source": [
        "#y = get_def_df()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzMOwSZTiUZl"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9EN3AWDAljQ"
      },
      "source": [
        "####################### -> Part 3 of the Code -> equivalent to the remove_dnr_pts and get_obs_df method\n",
        "#https://github.com/dhlee4/AcuteOrganFailureInterventionModel/blob/master/mimic_data_abstracter.py lines 45 -> 73\n",
        "#implement get_obs_df on line 75 to create cur_chart for line 45\n",
        "#cur_dnr_inst = cur_chart_df[cur_chart_df[\"ITEMID\"] == 223758]\n",
        "#cur_dnr_inst = cur_dnr_inst[cur_dnr_inst[\"VALUE\"].isin(cur_dnr_assertion)]\n",
        "#beginning implementation of get_obs_df method -> lines 75 - 111\n",
        "\n",
        "import pandas as pd\n",
        "def remove_dnr_pts(cur_chart = pd.DataFrame()):\n",
        "  # double check whether get_obs_df is called first and that this if statement is just as a safety measure, because if remove_dnr_pts is called first\n",
        "  #then, it would call get_obs_df which would call remove_dnr_points w/o the if statement, store it in cur_chart, and go through the non-if statement again\n",
        "  if(cur_chart.empty):\n",
        "    cur_chart = get_obs_df()\n",
        "\n",
        "  cur_dnr_assertion = [\"DNR (do not resuscitate)\", \"DNR / DNI\", \"Comfort measures only\", \"DNI (do not intubate)\"]\n",
        "  cur_dnr_inst = cur_chart[cur_chart[\"ITEMID\"] == 223758]\n",
        "  cur_dnr_inst = cur_dnr_inst[cur_dnr_inst[\"VALUE\"].isin(cur_dnr_assertion)]\n",
        "  cur_dnr_inst = cur_dnr_inst.groupby(\"ID\").TIME_OBS.agg(['min'])\n",
        "  cur_dnr_inst = cur_dnr_inst.rename(columns = {\"min\" : \"DNR_TIME\"})\n",
        "  cur_chart = cur_chart.merge(cur_dnr_inst, on = \"ID\", how = \"left\")\n",
        "  print(cur_dnr_inst.shape)\n",
        "  del cur_dnr_inst\n",
        "  cur_chart = cur_chart.head(int(len(cur_chart)/15)) # MAKE SURE TO COMMENT OUT CHANGED FROM 200!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "  #double check correct lambda function\n",
        "  cur_chart = cur_chart[cur_chart['DNR_TIME'].apply(lambda x: str(x) == \"NaT\" or cur_chart[\"DNR_TIME\"] > cur_chart[\"TIME_OBS\"])]\n",
        "  print(cur_chart.head())\n",
        "  cur_chart = cur_chart.drop(columns = [\"DNR_TIME\"])\n",
        "  pts_icu_stay = get_icu_stay() #continue this method, check whether to uncomment final code from get_obs_df\n",
        "  cur_chart = cur_chart.merge(pts_icu_stay, on = \"ID\")\n",
        "  cur_chart = cur_chart[cur_chart[\"INTIME\"] <= cur_chart[\"TIME_OBS\"]]\n",
        "  cur_chart = cur_chart[cur_chart[\"OUTTIME\"] >= cur_chart[\"TIME_OBS\"]]\n",
        "  cur_chart = cur_chart.drop(columns = [\"INTIME\", \"OUTTIME\"])\n",
        "  return cur_chart\n",
        "\n",
        "def get_obs_df():\n",
        "  import pandas as pd\n",
        "  cur_vital = pd.read_csv(\"physionet.org/files/mimiciii/1.4/CHARTEVENTS.csv.gz\", usecols = [\"HADM_ID\", \"VALUE\", \"CHARTTIME\", \"ITEMID\"], dtype = {\"HADM_ID\" : \"int32\", \"ITEMID\" : \"int32\"})\n",
        "  cur_vital = cur_vital.rename(columns = {\"HADM_ID\" : \"ID\", \"CHARTTIME\" : \"TIME_OBS\"})\n",
        "  cur_vital_AddedList = []\n",
        "  for i in range(len(cur_vital)):\n",
        "    cur_vital_AddedList.append(\"VITAL\") #this is the .alias lit() segment in SQL\n",
        "  cur_vital[\"SOURCE\"] = cur_vital_AddedList\n",
        "  print(cur_vital.info())\n",
        "  print(cur_vital.memory_usage())\n",
        "  cur_vital.head()\n",
        "\n",
        "  import pandas as pd\n",
        "  cur_lab = pd.read_csv(\"physionet.org/files/mimiciii/1.4/LABEVENTS.csv.gz\", usecols = [\"HADM_ID\", \"VALUE\", \"CHARTTIME\", \"ITEMID\"])\n",
        "  cur_lab = cur_lab.rename(columns = {\"HADM_ID\" : \"ID\", \"CHARTTIME\" : \"TIME_OBS\"})\n",
        "  cur_lab_AddedList = []\n",
        "  for i in range(len(cur_lab)):\n",
        "    cur_lab_AddedList.append(\"LAB\") #this is the .alias lit() segment in SQL\n",
        "  cur_lab[\"SOURCE\"] = cur_lab_AddedList\n",
        "  print(cur_lab.info())\n",
        "  print(cur_lab.memory_usage())\n",
        "  cur_lab.head()\n",
        "  cur_vital = cur_vital.head(int(len(cur_vital)/40)) # MAKE SURE TO COMMENT OUT  -> CHANGED FROM 40\n",
        "\n",
        "  merged_obs = pd.concat([cur_vital, cur_lab])\n",
        "  merged_obs.head()\n",
        "\n",
        "  #continuing get_obs_df method -> line 103 to 105\n",
        "  # TEST THIS PART OF THE CODE\n",
        "  merged_obs = pd.merge(merged_obs, get_icu_stay(), on='ID')\n",
        "  merged_obs[\"INTIME\"] = pd.to_datetime(merged_obs[\"INTIME\"])\n",
        "  merged_obs[\"OUTTIME\"] = pd.to_datetime(merged_obs[\"OUTTIME\"])\n",
        "  merged_obs[\"TIME_OBS\"] = pd.to_datetime(merged_obs[\"TIME_OBS\"])\n",
        "  merged_obs = merged_obs[merged_obs[\"INTIME\"] < merged_obs[\"OUTTIME\"]]\n",
        "  merged_obs = merged_obs[merged_obs[\"TIME_OBS\"] <= merged_obs[\"OUTTIME\"]].drop(columns = [\"INTIME\", \"OUTTIME\"])\n",
        "  merged_obs.head()\n",
        "  # DOUBLE CHECK WHETHER TO INCLUDE THIS, maybe comment out for testing\n",
        "  print(\"check!!!\")\n",
        "  merged_obs = remove_dnr_pts(cur_chart = merged_obs)\n",
        "\n",
        "  return merged_obs"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS4YhcAJQHVY"
      },
      "source": [
        "#implementing the get_icu_stay method, which is a helper method for get_obs_df (line 103 /216 -222)\n",
        "def get_icu_stay():\n",
        "  import pandas as pd\n",
        "  get_icu_stay_df = pd.read_csv(\"physionet.org/files/mimiciii/1.4/ICUSTAYS.csv.gz\", usecols = [\"HADM_ID\", \"INTIME\", \"OUTTIME\", \"DBSOURCE\"])\n",
        "  get_icu_stay_df = get_icu_stay_df[get_icu_stay_df[\"DBSOURCE\"] == \"metavision\"]\n",
        "  get_icu_stay_df = get_icu_stay_df.rename(columns = {\"HADM_ID\" : \"ID\"})\n",
        "  get_icu_stay_df[\"INTIME\"] = pd.to_datetime(get_icu_stay_df[\"INTIME\"])\n",
        "  get_icu_stay_df[\"OUTTIME\"] = pd.to_datetime(get_icu_stay_df[\"OUTTIME\"])\n",
        "  get_icu_stay_df.head()\n",
        "  return get_icu_stay_df"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5sCB--Px_rY"
      },
      "source": [
        "#obsDf = get_obs_df() use only top comment\n",
        "#cur_chart = remove_dnr_pts(obsDf)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEwvhPLIuX9-"
      },
      "source": [
        "#obsDf.head()\n",
        "#check why dbsource x and y exist and are not merged -> should be baesd on the icu_stay new code for the method"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H2TnJ0f0dzx"
      },
      "source": [
        "def get_action_df():\n",
        "  cur_med = pd.read_csv(\"physionet.org/files/mimiciii/1.4/INPUTEVENTS_MV.csv.gz\", usecols = [\"HADM_ID\", \"STARTTIME\", \"ITEMID\"])\n",
        "  cur_med_AddedList = []\n",
        "  for i in range(len(cur_med)):\n",
        "    cur_med_AddedList.append(\"MED\") #this is the .alias lit() segment in SQL\n",
        "  cur_med[\"SOURCE\"] = cur_med_AddedList\n",
        "  cur_med = cur_med.rename(columns = {\"HADM_ID\" : \"ID\", \"STARTTIME\" : \"TIME_OBS\"})\n",
        "\n",
        "  cur_cpt = pd.read_csv(\"physionet.org/files/mimiciii/1.4/CPTEVENTS.csv.gz\", usecols = [\"HADM_ID\", \"CHARTDATE\", \"CPT_NUMBER\"])\n",
        "  cur_cpt_AddedList = []\n",
        "  for i in range(len(cur_cpt)):\n",
        "    cur_cpt_AddedList.append(\"CPT\") #this is the .alias lit() segment in SQL\n",
        "  cur_cpt[\"SOURCE\"] = cur_cpt_AddedList\n",
        "  cur_cpt = cur_cpt.rename(columns = {\"HADM_ID\" : \"ID\", \"CHARTDATE\" : \"TIME_OBS\", \"CPT_NUMBER\" : \"ITEMID\"})\n",
        "\n",
        "  cur_proc = pd.read_csv(\"physionet.org/files/mimiciii/1.4/PROCEDUREEVENTS_MV.csv.gz\", usecols = [\"HADM_ID\", \"STARTTIME\", \"ITEMID\"])\n",
        "  cur_proc_AddedList = []\n",
        "  for i in range(len(cur_proc)):\n",
        "    cur_proc_AddedList.append(\"PROC\") #this is the .alias lit() segment in SQL\n",
        "  cur_proc[\"SOURCE\"] = cur_proc_AddedList\n",
        "  cur_proc = cur_proc.rename(columns = {\"HADM_ID\" : \"ID\", \"STARTTIME\" : \"TIME_OBS\"})\n",
        "\n",
        "  ret_df = None\n",
        "  for cur_df in [cur_med,cur_cpt,cur_proc]:\n",
        "    #double check this if statement to ensure same purpose\n",
        "    if(not cur_df.empty):\n",
        "      if ret_df is not None:\n",
        "        ret_df = pd.concat([ret_df, cur_df])\n",
        "      else:\n",
        "        ret_df = cur_df\n",
        "  return ret_df.drop_duplicates()\n",
        "      \n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7k01YDp1hmA"
      },
      "source": [
        "#x = get_action_df()\n",
        "#x.head()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbMtO1ak4-LB"
      },
      "source": [
        "def get_terminal_df():\n",
        "  cur_df = pd.read_csv(\"physionet.org/files/mimiciii/1.4/DIAGNOSES_ICD.csv.gz\", usecols = [\"HADM_ID\", \"ICD9_CODE\"])\n",
        "  ret_df = cur_df.rename(columns = {\"HADM_ID\" : \"ID\"})\n",
        "  return ret_df"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF4pCkuH1kFi"
      },
      "source": [
        "#y = get_terminal_df()\n",
        "#y.head()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLujyfsG6A8x"
      },
      "source": [
        "def get_hospital_death():  \n",
        "  ret_df = pd.read_csv(\"physionet.org/files/mimiciii/1.4/ADMISSIONS.csv.gz\", usecols = [\"HADM_ID\", \"HOSPITAL_EXPIRE_FLAG\"])\n",
        "  ret_df = ret_df.rename(columns = {\"HADM_ID\" : \"ID\", \"HOSPITAL_EXPIRE_FLAG\" : \"IS_DEAD\"})\n",
        "  return ret_df"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9abV3Ao6toN"
      },
      "source": [
        "#z = get_hospital_death()\n",
        "#z.head()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDqgF1RE6vTs"
      },
      "source": [
        "def get_action_itemids():\n",
        "  cur_target_action_cpt_rdd = spark.read.option(\"header\",True) \\\n",
        "     .csv(\"/content/physionet.org/files/mimiciii/1.4/CPTEVENTS.csv.gz\") \\\n",
        "            .where(col(\"CHARTDATE\").isNotNull()).where(\"SECTIONHEADER <> 'Evaluation and management'\")\\\n",
        "            .select(\"CPT_CD\").rdd.flatMap(list).map(lambda x: {\"SOURCE\":\"CPT\",\"ITEMID\":x})\n",
        "\n",
        "  cur_target_action_med_rdd = spark.read.option(\"header\",True) \\\n",
        "     .csv(\"/content/physionet.org/files/mimiciii/1.4/D_ITEMS.csv.gz\")\\\n",
        "            .where(\"DBSOURCE == 'metavision' and LINKSTO = 'inputevents_mv'\")\\\n",
        "            .where(col(\"CATEGORY\").isin([\"Dialysis\", \"2-Ventilation\", \"Blood Products/Colloids\", \"4-Procedures\"\n",
        "                                                        , \"1-Intubation/Extubation\",\"3-Significant Events\", \"Medications\"]))\\\n",
        "            .select(\"ITEMID\").rdd.flatMap(list).map(lambda x:{\"SOURCE\":\"MED\",\"ITEMID\":x})\n",
        "\n",
        "  cur_target_action_proc_rdd = spark.read.option(\"header\",True) \\\n",
        "     .csv(\"/content/physionet.org/files/mimiciii/1.4/D_ITEMS.csv.gz\")\\\n",
        "            .where(\"DBSOURCE == 'metavision' and LINKSTO = 'procedureevents_mv'\")\\\n",
        "            .where(col(\"CATEGORY\").isin([\"Dialysis\", \"2-Ventilation\", \"Blood Products/Colloids\", \"4-Procedures\"\n",
        "                                                        , \"1-Intubation/Extubation\",\"3-Significant Events\", \"Medications\"]))\\\n",
        "            .select(\"ITEMID\").rdd.flatMap(list).map(lambda x:{\"SOURCE\":\"PROC\",\"ITEMID\":x})\n",
        "\n",
        "  ret_df = spark.sparkContext.union([cur_target_action_cpt_rdd,cur_target_action_med_rdd,cur_target_action_proc_rdd]).\\\n",
        "            toDF().distinct()\n",
        "  return ret_df"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS4TUIsAp8wk"
      },
      "source": [
        "#x = get_action_itemids()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T07TK23rQHk"
      },
      "source": [
        "#pandasDF = x.toPandas()\n",
        "#print(pandasDF)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jc59jsquht6"
      },
      "source": [
        "#DBG methods were coppied and pasted; irrelevant for this purpose    \n",
        "def get_target_ids_list_DBG(self):\n",
        "  return self.cur_target_id\n",
        "\n",
        "def get_obs_df_DBG(self,target_id):\n",
        "  if type(self) == data_abstracter:\n",
        "      raise NotImplementedError(\"Method need to be called in sub-class but currently called in base class\")\n",
        "\n",
        "  '''\n",
        "  :param target_id: target ID used for debug run\n",
        "  :return:\n",
        "  '''\n",
        "  import pyspark\n",
        "  #maybe this can be moved to main method?\n",
        "  try:\n",
        "      return self.spark.read.parquet(self.cached_obs_df+self.dbg_post_fix)\n",
        "  except pyspark.sql.utils.AnalysisException as ex:\n",
        "      template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
        "      message = template.format(type(ex).__name__, ex.args)\n",
        "      self.logger.info(message)\n",
        "      self.logger.info(\"PROCESS\")\n",
        "  from pyspark.sql.functions import col\n",
        "  cur_df = self.get_obs_df()\n",
        "  cur_df.where(col(\"ID\").isin(target_id)).write.save(self.cached_obs_df+self.dbg_post_fix)\n",
        "  return self.spark.read.parquet(self.cached_obs_df+self.dbg_post_fix)\n",
        "\n",
        "def get_action_df_DBG(self,target_id):\n",
        "  if type(self) == data_abstracter:\n",
        "      raise NotImplementedError(\"Method need to be called in sub-class but currently called in base class\")\n",
        "\n",
        "  '''\n",
        "  :param target_id:target ID used for debug run\n",
        "  :return:\n",
        "  '''\n",
        "  import pyspark\n",
        "\n",
        "  try:\n",
        "      return self.spark.read.parquet(self.cached_action_df+self.dbg_post_fix)\n",
        "  except pyspark.sql.utils.AnalysisException as ex:\n",
        "      template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
        "      message = template.format(type(ex).__name__, ex.args)\n",
        "      self.logger.info(message)\n",
        "      self.logger.info(\"PROCESS\")\n",
        "  from pyspark.sql.functions import col\n",
        "  cur_df = self.get_action_df()\n",
        "  cur_df.where(col(\"ID\").isin(target_id)).write.save(self.cached_action_df+self.dbg_post_fix)\n",
        "  return self.spark.read.parquet(self.cached_action_df+self.dbg_post_fix)\n",
        "\n",
        "def get_terminal_df_DBG(self,target_id):\n",
        "  if type(self) == data_abstracter:\n",
        "      raise NotImplementedError(\"Method need to be called in sub-class but currently called in base class\")\n",
        "\n",
        "  '''\n",
        "  :param target_id:target ID used for debug run\n",
        "  :return:\n",
        "  '''\n",
        "  import pyspark\n",
        "  try:\n",
        "      return self.spark.read.parquet(self.cached_terminal_df+self.dbg_post_fix)\n",
        "  except pyspark.sql.utils.AnalysisException as ex:\n",
        "      template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
        "      message = template.format(type(ex).__name__, ex.args)\n",
        "      self.logger.info(message)\n",
        "      self.logger.info(\"PROCESS\")\n",
        "  from pyspark.sql.functions import col\n",
        "  cur_df = self.get_terminal_df()\n",
        "  cur_df.where(col(\"ID\").isin(target_id)).write.save(self.cached_terminal_df+self.dbg_post_fix)\n",
        "  return self.spark.read.parquet(self.cached_terminal_df+self.dbg_post_fix)\n",
        "\n",
        "def get_target_ids_list():\n",
        "  from pyspark.context import SparkContext\n",
        "  from pyspark.sql.session import SparkSession\n",
        "  sc = SparkContext.getOrCreate();\n",
        "  spark = SparkSession(sc)\n",
        "  obs_df = get_obs_df()\n",
        "  obs_df.VALUE = obs_df.VALUE.astype(str)\n",
        "  cur_target_id_list = spark.createDataFrame(obs_df)\n",
        "  cur_target_id_list = cur_target_id_list.select(\"ID\").distinct().rdd.flatMap(list).collect()\n",
        "  return cur_target_id_list"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIgLagVyu4Mp"
      },
      "source": [
        "def get_target_test_id(tr_te_prop = 0.1):\n",
        "\n",
        "        import json\n",
        "        cur_list = get_target_ids_list()\n",
        "        from random import shuffle\n",
        "        shuffle(cur_list)\n",
        "        target_test_id_list = cur_list[:int(len(cur_list)*tr_te_prop)]\n",
        "        return target_test_id_list\n",
        "\n",
        "def get_target_tr_val_id(tr_val_prop = 0.2):\n",
        "        import json\n",
        "        cur_all_ids = get_target_ids_list()\n",
        "        cur_test_list = get_target_test_id()\n",
        "        from random import shuffle\n",
        "        cur_tr_ids = list(set(cur_all_ids).difference(set(cur_test_list)))\n",
        "        shuffle(cur_tr_ids)\n",
        "        target_val_ids = cur_tr_ids[:int(len(cur_tr_ids)*tr_val_prop)]\n",
        "        target_train_ids = cur_tr_ids[int(len(cur_tr_ids)*tr_val_prop):]\n",
        "        return {\"TR\":target_train_ids, \"VAL\":target_val_ids}\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HZoHXkUYNtm"
      },
      "source": [
        "def mimic_preprocess_init():\n",
        "  #data_preprocesser_init() -> does nothing\n",
        "  return mimic_data_abstracter_init()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53YpmA88YqcC"
      },
      "source": [
        "def data_abstracter_init():\n",
        "  import pyspark\n",
        "  from pyspark.sql.session import SparkSession\n",
        "  from pyspark import SparkConf\n",
        "  def_df = get_def_df()\n",
        "  obs_df = get_obs_df()\n",
        "\n",
        "  from pyspark.sql.functions import col\n",
        "  cur_target_id = get_target_ids_list()\n",
        "  obs_df = obs_df[obs_df[\"ID\"].isin(cur_target_id)]\n",
        "  action_df = get_action_df()\n",
        "  action_df = action_df[action_df[\"ID\"].isin(cur_target_id)]\n",
        "  terminal_df = get_terminal_df()\n",
        "  terminal_df = terminal_df[terminal_df[\"ID\"].isin(cur_target_id)]\n",
        "\n",
        "  return cur_target_id, obs_df, action_df, terminal_df, def_df"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4zIa4pWYyUI"
      },
      "source": [
        "def mimic_data_abstracter_init():\n",
        "  return data_abstracter_init()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDo_QWAnZIP0"
      },
      "source": [
        "#main run\n",
        "#output deleted for readability (VERY long)\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql.session import SparkSession\n",
        "import pyspark\n",
        "cur_target_id, obs_df, action_df, terminal_df, def_df = mimic_preprocess_init()\n",
        "#data_run_experiment_init() -> does nothing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLIoSuFDZ5Od"
      },
      "source": [
        "def run_preprocessor(obs_df = obs_df):\n",
        "  from pyspark.context import SparkContext\n",
        "  from pyspark.sql.session import SparkSession\n",
        "  from pyspark.sql.functions import col,split,struct, date_add\n",
        "  sc = SparkContext.getOrCreate();\n",
        "  spark = SparkSession(sc)\n",
        "  obs_df.VALUE = obs_df.VALUE.astype(str)\n",
        "  obs_df = spark.createDataFrame(obs_df)\n",
        "  cur_obs_bf_dropna = obs_df.select(\"ID\", \"ITEMID\", \"VALUE\", \"TIME_OBS\",\n",
        "                                          split(\"TIME_OBS\", \"\\ \").getItem(0).alias(\"DATE_OBS\")) \\\n",
        "            .withColumn(\"TIME_SPAN\", struct(col(\"DATE_OBS\").cast(\"timestamp\").alias(\"TIME_FROM\") \\\n",
        "                                            , date_add(\"DATE_OBS\", 1).cast(\"timestamp\").alias(\"TIME_TO\")))\n",
        "\n",
        "\n",
        "  cur_obs = cur_obs_bf_dropna.dropna()\n",
        "  return run_remaining(cur_obs)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2Vtvy-MipYU"
      },
      "source": [
        "def run_remaining(cur_obs):\n",
        "  from pyspark.sql.functions import count,col\n",
        "  cur_obs.groupBy(\"ITEMID\").agg(count(\"*\").alias(\"cnt\")).orderBy(col(\"cnt\").desc()).show(300)\n",
        "  num_cat_tagged = num_cat_tagger(cur_obs)\n",
        "  cat_raw_filtered, cat_voca_list = cat_frequency_filter(num_cat_tagged.where(\"IS_CAT == 1\"))\n",
        "  num_raw_filtered, num_ref_list = num_iqr_filter(num_cat_tagged.where(\"IS_CAT == 0\"))\n",
        "  num_instance = count_instance(cat_raw_filtered, num_raw_filtered)\n",
        "  for cur_th in th_range:\n",
        "    REPARTITION_CONST = 200\n",
        "    cat_filtered = availability_filter(cat_raw_filtered, num_instance, availability_th=cur_th,\n",
        "                                        REPARTITION_CONST=REPARTITION_CONST)\n",
        "\n",
        "    num_filtered = availability_filter(num_raw_filtered, num_instance, availability_th=cur_th,\n",
        "                                        REPARTITION_CONST=REPARTITION_CONST)\n",
        "\n",
        "\n",
        "    cat_featurized = cat_featurizer(cat_filtered, voca_df=cat_voca_list\\\n",
        "                                                          , REPARTITION_CONST=REPARTITION_CONST)\n",
        "\n",
        "\n",
        "    num_featurized = num_featurizer(num_filtered, ref_df=num_ref_list\\\n",
        "                                                          , REPARTITION_CONST=REPARTITION_CONST)\n",
        "\n",
        "    merged_all = feature_aggregator(num_featurized, cat_featurized\\\n",
        "                                                          , REPARTITION_CONST=REPARTITION_CONST)\n",
        "    target_rdd, target_schema, feature_column = flattener_df_prep(merged_all)\n",
        "    cur_df = spark.createDataFrame(target_rdd, target_schema).persist()\n",
        "\n",
        "  return cur_df\n",
        "  "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-DF7CUOwqry"
      },
      "source": [
        "#helper methods for run_remaining\n",
        "from pyspark.sql.functions import col,when,lit\n",
        "def num_cat_tagger(data_frame, inputCol=\"VALUE\", outputCol=\"IS_CAT\",labelCol=\"ITEMID\",REPARTITION_CONST = None,nominal_th = 2):\n",
        "        #DL0411: Output should be list of features with corresponding num/cat instances\n",
        "        from pyspark.sql.functions import size,collect_set\n",
        "        if REPARTITION_CONST is None:\n",
        "            get_nominal_var = data_frame.repartition(labelCol).groupBy(labelCol).agg(size(collect_set(inputCol)).alias(\"value_cnt\"))\\\n",
        "                         .where(\"value_cnt<={0}\".format(nominal_th)).select(labelCol).rdd.flatMap(list).collect()\n",
        "        else:\n",
        "            get_nominal_var = data_frame.repartition(REPARTITION_CONST).groupBy(labelCol).agg(size(collect_set(inputCol)).alias(\"value_cnt\"))\\\n",
        "                         .where(\"value_cnt<={0}\".format(nominal_th)).select(labelCol).rdd.flatMap(list).collect()\n",
        "        data_frame.show()\n",
        "\n",
        "        if REPARTITION_CONST is None:\n",
        "            ret_data_frame = data_frame.withColumn(outputCol,\n",
        "                when((col(inputCol).rlike('^(?!-0?(\\.0+)?(E|$))-?(0|[1-9]\\d*)?(\\.\\d+)?(?<=\\d)(E-?(0|[1-9]\\d*))?$'))&\n",
        "                     (~col(labelCol).isin(get_nominal_var)),lit(\"0\"))\\\n",
        "                .otherwise(lit(\"1\")))\n",
        "        else:\n",
        "            ret_data_frame = data_frame.withColumn(outputCol,\n",
        "                when((col(inputCol).rlike('^(?!-0?(\\.0+)?(E|$))-?(0|[1-9]\\d*)?(\\.\\d+)?(?<=\\d)(E-?(0|[1-9]\\d*))?$'))&\n",
        "                     (~col(labelCol).isin(get_nominal_var)),lit(\"0\"))\\\n",
        "                .otherwise(lit(\"1\"))).repartition(REPARTITION_CONST)\n",
        "        return ret_data_frame\n",
        "\n",
        "def cat_frequency_filter(data_frame,threshold_lb = 0,threshold_ub = 1,inputCol=\"VALUE\",labelCol=\"ITEMID\",REPARTITION_CONST=None):\n",
        "        from pyspark.sql.functions import count,monotonically_increasing_id\n",
        "        label_count = data_frame.groupBy(labelCol).agg(count(\"*\").alias(\"label_count\"))\n",
        "        if REPARTITION_CONST is not None:\n",
        "            label_count = label_count.repartition(REPARTITION_CONST).persist()\n",
        "        if REPARTITION_CONST is None:\n",
        "            cur_freq = data_frame.groupBy(labelCol,inputCol).agg(count(\"*\").alias(\"indiv_count\")).join(label_count,labelCol)\\\n",
        "                                .withColumn(\"cat_freq\",col(\"indiv_count\")/col(\"label_count\"))\n",
        "        else:\n",
        "            cur_freq = data_frame.groupBy(labelCol,inputCol).agg(count(\"*\").alias(\"indiv_count\")).repartition(REPARTITION_CONST).join(label_count,labelCol)\\\n",
        "                                .withColumn(\"cat_freq\",col(\"indiv_count\")/col(\"label_count\")).repartition(REPARTITION_CONST)\n",
        "        cur_freq = cur_freq.where((col(\"cat_freq\")>=threshold_lb) & (col(\"cat_freq\")<=threshold_ub)).drop(\"cat_freq\")\n",
        "        #cur_freq.orderBy(col(labelCol),-1*col(\"cat_freq\")).show(500)\n",
        "\n",
        "        ret_data_frame = data_frame.join(cur_freq,[inputCol,labelCol]).drop(\"indiv_count\").drop(\"label_count\")\n",
        "\n",
        "        if REPARTITION_CONST is not None:\n",
        "            ret_data_frame = ret_data_frame.repartition(REPARTITION_CONST)\n",
        "        ret_voca = ret_data_frame.select(\"ITEMID\",\"VALUE\").distinct()\n",
        "        ret_voca = ret_voca.rdd.map(lambda x: (x.ITEMID,x.VALUE)).zipWithUniqueId().map(lambda x: {\"idx\":x[1], \"ITEMID\":x[0][0], \"VALUE\":x[0][1]}).toDF()\n",
        "        if REPARTITION_CONST is not None:\n",
        "            ret_voca = ret_voca.repartition(REPARTITION_CONST)\n",
        "\n",
        "        return (ret_data_frame, ret_voca)\n",
        "\n",
        "def num_iqr_filter(data_frame, inputCol=\"VALUE\",labelCol=\"ITEMID\",REPARTITION_CONST = None,sc=None):\n",
        "        from pyspark.sql.window import Window\n",
        "        from pyspark.sql.functions import abs,percent_rank,row_number,collect_list,udf,struct,count,avg,stddev_pop\n",
        "        from pyspark.sql.types import MapType,StringType,DoubleType\n",
        "        if REPARTITION_CONST is None:\n",
        "            value_order = Window.partitionBy(labelCol).orderBy(col(inputCol).cast(\"float\"))\n",
        "            Q1_percentile = Window.partitionBy(labelCol).orderBy(abs(0.25-col(\"percentile\")))\n",
        "            Q2_percentile = Window.partitionBy(labelCol).orderBy(abs(0.5-col(\"percentile\")))\n",
        "            Q3_percentile = Window.partitionBy(labelCol).orderBy(abs(0.75-col(\"percentile\")))\n",
        "            percent_data_frame = data_frame.select(labelCol, inputCol, percent_rank().over(value_order).alias(\"percentile\"))\n",
        "            Q1_data_frame = percent_data_frame.withColumn(\"Q1_rn\",row_number().over(Q1_percentile)).where(\"Q1_rn == 1\")\\\n",
        "                            .select(labelCol,inputCol,lit(\"Q1\").alias(\"quantile\"))\n",
        "            Q2_data_frame = percent_data_frame.withColumn(\"Q2_rn\",row_number().over(Q2_percentile)).where(\"Q2_rn == 1\")\\\n",
        "                            .select(labelCol,inputCol,lit(\"Q2\").alias(\"quantile\"))\n",
        "            Q3_data_frame = percent_data_frame.withColumn(\"Q3_rn\",row_number().over(Q3_percentile)).where(\"Q3_rn == 1\")\\\n",
        "                            .select(labelCol,inputCol,lit(\"Q3\").alias(\"quantile\"))\n",
        "            merge_all = Q1_data_frame.unionAll(Q2_data_frame).unionAll(Q3_data_frame).persist()\n",
        "\n",
        "\n",
        "            udf_parse_list_to_map = udf(lambda maps: dict(list(tuple(x) for x in maps)),MapType(StringType(),StringType()))\n",
        "\n",
        "            aggregate_quantiles = merge_all.groupBy(labelCol).agg(collect_list(struct(\"quantile\",inputCol)).alias(\"quant_info\"))\n",
        "\n",
        "            aggregate_quantiles = aggregate_quantiles.select(labelCol,udf_parse_list_to_map(\"quant_info\").alias(\"quant_info\"))\n",
        "\n",
        "            iqr_data_frame = aggregate_quantiles.withColumn(\"Q1\",col(\"quant_info\").getItem(\"Q1\").cast(\"float\"))\\\n",
        "                .withColumn(\"Q2\",col(\"quant_info\").getItem(\"Q2\").cast(\"float\"))\\\n",
        "                .withColumn(\"Q3\",col(\"quant_info\").getItem(\"Q3\").cast(\"float\"))\n",
        "\n",
        "        else:\n",
        "            cur_label_list = data_frame.select(labelCol).distinct().rdd.flatMap(list).collect()\n",
        "            cur_iqr_list = list()\n",
        "            cnt = -1\n",
        "            for cur_item in cur_label_list:\n",
        "                cnt = cnt+1\n",
        "                data_frame.where(col(labelCol) == cur_item).registerTempTable(\"cur_table\")\n",
        "                cur_iqr = sc.sql(\"select {0}, percentile_approx({1},0.25) as Q1, percentile_approx({2},0.5) as Q2, percentile_approx({3},0.75) as Q3 from cur_table group by {4}\".format(labelCol,inputCol,inputCol,inputCol,labelCol)).first().asDict()\n",
        "                cur_iqr_list.append(cur_iqr)\n",
        "                sc.catalog.dropTempView(\"cur_table\")\n",
        "                #percent_data_frame = data_frame.select(labelCol, inputCol, percent_rank().over(value_order).alias(\"percentile\")).repartition(REPARTITION_CONST).cache().checkpoint()\n",
        "            iqr_data_frame = sc.createDataFrame(cur_iqr_list).repartition(REPARTITION_CONST)\n",
        "\n",
        "\n",
        "        if REPARTITION_CONST is None:\n",
        "            iqr_data_frame = iqr_data_frame.withColumn(\"IQR\",col(\"Q3\")-col(\"Q1\"))\\\n",
        "                                       .withColumn(\"LB\",col(\"Q1\")-1.5*col(\"IQR\"))\\\n",
        "                                       .withColumn(\"UB\",col(\"Q3\")+1.5*col(\"IQR\"))\\\n",
        "                                       .select(labelCol,\"LB\",\"UB\")\n",
        "        else:\n",
        "            iqr_data_frame = iqr_data_frame.withColumn(\"IQR\",col(\"Q3\")-col(\"Q1\"))\\\n",
        "                                       .withColumn(\"LB\",col(\"Q1\")-1.5*col(\"IQR\"))\\\n",
        "                                       .withColumn(\"UB\",col(\"Q3\")+1.5*col(\"IQR\"))\\\n",
        "                                       .select(labelCol,\"LB\",\"UB\").repartition(REPARTITION_CONST).persist()\n",
        "        if REPARTITION_CONST is None:\n",
        "\n",
        "            ret_data_frame = data_frame.repartition(labelCol).join(iqr_data_frame,labelCol).where((col(\"LB\").cast(\"float\") <= col(inputCol).cast(\"float\")) & (col(\"UB\").cast(\"float\")>=col(inputCol).cast(\"float\")))\\\n",
        "                                                                 .drop(\"LB\").drop(\"UB\").persist()\n",
        "            ref_df = ret_data_frame.repartition(labelCol).groupBy(labelCol)\\\n",
        "                               .agg(count(inputCol).alias(\"ref_count\"),avg(inputCol).alias(\"ref_avg\"),stddev_pop(inputCol).alias(\"ref_std\")).persist()\n",
        "\n",
        "            return (ret_data_frame, ref_df)\n",
        "        else:\n",
        "              ret_data_frame = data_frame.join(iqr_data_frame,labelCol).where((col(\"LB\").cast(\"float\") <= col(inputCol).cast(\"float\")) & (col(\"UB\").cast(\"float\")>=col(inputCol).cast(\"float\")))\\\n",
        "                                                                  .drop(\"LB\").drop(\"UB\").repartition(REPARTITION_CONST)\n",
        "              ref_df = ret_data_frame.groupBy(labelCol)\\\n",
        "                                .agg(count(inputCol).alias(\"ref_count\"),avg(inputCol).alias(\"ref_avg\"),stddev_pop(inputCol).alias(\"ref_std\")).repartition(REPARTITION_CONST)\n",
        "\n",
        "              return (ret_data_frame, ref_df)\n",
        "\n",
        "def count_instance(raw_feature1, raw_feature2 = None):\n",
        "        from pyspark.sql.functions import collect_set,size\n",
        "        raw1_distinct_instance = raw_feature1\\\n",
        "            .select(\"ID\",\"TIME_SPAN\").withColumn(\"T1\",col(\"TIME_SPAN\").TIME_FROM.cast(\"timestamp\"))\\\n",
        "            .withColumn(\"T2\",col(\"TIME_SPAN\").TIME_TO.cast(\"timestamp\")).select(\"ID\",\"T1\",\"T2\")\n",
        "        if raw_feature2 is not None:\n",
        "            raw2_distinct_instance = raw_feature2\\\n",
        "                .select(\"ID\",\"TIME_SPAN\").withColumn(\"T1\",col(\"TIME_SPAN\").TIME_FROM.cast(\"timestamp\"))\\\n",
        "                .withColumn(\"T2\",col(\"TIME_SPAN\").TIME_TO.cast(\"timestamp\")).select(\"ID\",\"T1\",\"T2\")\n",
        "        else:\n",
        "            raw2_distinct_instance = None\n",
        "        if raw2_distinct_instance is not None:\n",
        "            all_inst = raw1_distinct_instance.unionAll(raw2_distinct_instance)\n",
        "        else:\n",
        "            all_inst = raw1_distinct_instance\n",
        "\n",
        "        return all_inst.distinct().groupBy(\"ID\").agg(collect_set(\"T1\").alias(\"collected_set\")).select(\"ID\",size(\"collected_set\").alias(\"list_size\")).rdd.map(lambda x: x.list_size).reduce(lambda a,b:a+b)\n",
        "\n",
        "def availability_filter(data_frame,n_inst = None, availability_th = 0.80,labelCol=\"ITEMID\",idCol=\"ID\",timeCol=\"TIME_SPAN\",REPARTITION_CONST=None):\n",
        "        from pyspark.sql.functions import count\n",
        "        if not n_inst:\n",
        "            total_cnt = data_frame.select(idCol,timeCol).distinct().count()\n",
        "\n",
        "        else:\n",
        "            total_cnt = n_inst\n",
        "        target_label_set = data_frame.select(idCol,timeCol,labelCol).distinct()\n",
        "\n",
        "        if REPARTITION_CONST is None:\n",
        "            target_label_set = target_label_set.groupBy(labelCol).agg((count(\"*\")/float(total_cnt)).alias(\"freq\"))\n",
        "        else:\n",
        "            target_label_set = target_label_set.repartition(REPARTITION_CONST)\\\n",
        "                .groupBy(labelCol).agg((count(\"*\")/float(total_cnt)).alias(\"freq\"))\n",
        "\n",
        "        target_label_set.orderBy(col(\"freq\").desc()).show()\n",
        "\n",
        "        if REPARTITION_CONST is not None:\n",
        "            target_label_set = target_label_set.repartition(REPARTITION_CONST)\n",
        "        target_label_set = target_label_set.where(col(\"freq\")>=availability_th).select(labelCol).rdd.flatMap(list).collect()\n",
        "        if len(target_label_set) == 0:\n",
        "            return\n",
        "        ret_data_frame = data_frame.where(col(labelCol).isin(target_label_set))\n",
        "\n",
        "        return ret_data_frame\n",
        "\n",
        "def cat_featurizer(data_frame, voca_df, inputCol=\"VALUE\",labelCol=\"ITEMID\",outputCol=\"cat_features\",REPARTITION_CONST = None):\n",
        "        def prep_cat_dict(avail, pos):  # internal\n",
        "            ret_dict_key = list(map(lambda x: \"C_\" + str(x), avail))\n",
        "            ret_dict = dict(zip(ret_dict_key, [0.0] * len(ret_dict_key)))\n",
        "            pos_dict_key = list(map(lambda x: \"C_\" + str(x), pos))\n",
        "            update_dict = dict(zip(pos_dict_key, [1.0] * len(pos_dict_key)))\n",
        "            ret_dict.update(update_dict)\n",
        "            return ret_dict\n",
        "\n",
        "        from pyspark.sql.functions import udf,collect_set\n",
        "        from pyspark.sql.types import MapType, StringType, DoubleType\n",
        "        if not data_frame:\n",
        "            return\n",
        "\n",
        "        all_var = voca_df.groupBy(labelCol).agg(collect_set(\"idx\").alias(\"AVAIL_LIST\"))\n",
        "        ret_data_frame = data_frame.join(voca_df,[inputCol,labelCol]).drop(\"VALUE\").withColumnRenamed(\"idx\",\"VALUE\")\n",
        "        if REPARTITION_CONST is not None:\n",
        "            ret_data_frame = ret_data_frame.repartition(REPARTITION_CONST)\n",
        "        ret_data_frame = value_aggregator(ret_data_frame).join(all_var,\"ITEMID\")\n",
        "\n",
        "        udf_prep_cat_dict = udf(prep_cat_dict, MapType(StringType(),DoubleType()))\n",
        "        ret_data_frame = ret_data_frame.withColumn(\"cat_features\",udf_prep_cat_dict(\"AVAIL_LIST\",\"VALUE_LIST\"))\n",
        "        return ret_data_frame\n",
        "\n",
        "def value_aggregator(data_frame, aggregateCols = [\"ID\",\"TIME_SPAN\",\"ITEMID\"], catmarkerCol=\"IS_CAT\", inputCol = \"VALUE\", outputCol=\"VALUE_LIST\"):\n",
        "        from pyspark.sql.functions import collect_set, collect_list\n",
        "        cat_data_agg_frame = data_frame.where(col(catmarkerCol) == 1).groupBy(aggregateCols+[catmarkerCol])\\\n",
        "                                                                     .agg(collect_set(inputCol).alias(outputCol))\n",
        "        num_data_agg_frame = data_frame.where(col(catmarkerCol) == 0).groupBy(aggregateCols+[catmarkerCol])\\\n",
        "                                                                     .agg(collect_list(inputCol).alias(outputCol))\n",
        "        return cat_data_agg_frame.unionAll(num_data_agg_frame)\n",
        "\n",
        "def num_featurizer(data_frame, ref_df=None, featurize_process = [\"summary_stat\",\"sustainment_q\"], inputCol=\"VALUE\",labelCol=\"ITEMID\",outputCol=\"num_features\",REPARTITION_CONST=None):\n",
        "        from pyspark.sql.functions import udf,array\n",
        "        from pyspark.sql.types import StringType, DoubleType, MapType\n",
        "        if not data_frame:\n",
        "            return\n",
        "        ret_data_frame = value_aggregator(data_frame)\n",
        "        if REPARTITION_CONST is not None:\n",
        "            ret_data_frame = ret_data_frame\n",
        "        if \"summary_stat\" in featurize_process:\n",
        "            udf_summary_stat = udf(calc_summary_stat,MapType(StringType(),DoubleType()))\n",
        "            ret_data_frame = ret_data_frame.withColumn(\"summary_stat\",udf_summary_stat(inputCol+\"_LIST\",labelCol))\n",
        "            if REPARTITION_CONST is not None:\n",
        "                ret_data_frame = ret_data_frame\n",
        "\n",
        "        if \"sustainment_q\" in featurize_process:\n",
        "            udf_sustainment_quant = udf(sustainment_quantifier,MapType(StringType(),DoubleType()))\n",
        "            ret_data_frame = ret_data_frame.join(ref_df,labelCol).withColumn(\"sustainment_q\",udf_sustainment_quant(\"summary_stat\",labelCol,\"ref_avg\",\"ref_std\",\"ref_count\")).drop(\"ref_avg\").drop(\"ref_std\").drop(\"ref_count\")\n",
        "            if REPARTITION_CONST is not None:\n",
        "                ret_data_frame = ret_data_frame\n",
        "        udf_merge_dict_all = udf(merge_dict_all,MapType(StringType(),DoubleType()))\n",
        "        ret_data_frame = ret_data_frame.withColumn(outputCol,udf_merge_dict_all(array(featurize_process)))\n",
        "        return ret_data_frame\n",
        "\n",
        "def calc_summary_stat(x,labelCol):\n",
        "        import numpy as np\n",
        "        cur_array = np.array(x,dtype=float)\n",
        "        ret_dict = dict()\n",
        "        ret_dict[\"N_{0}_avg\".format(labelCol)] = float(np.average(cur_array))\n",
        "        ret_dict[\"N_{0}_min\".format(labelCol)] = float(np.min(cur_array))\n",
        "        ret_dict[\"N_{0}_max\".format(labelCol)] = float(np.max(cur_array))\n",
        "        ret_dict[\"N_{0}_std\".format(labelCol)] = float(np.std(cur_array))\n",
        "        ret_dict[\"N_{0}_count\".format(labelCol)] = float(np.shape(cur_array)[0])\n",
        "        return ret_dict\n",
        "\n",
        "def sustainment_quantifier(x,cur_label,ref_avg, ref_std, ref_count):\n",
        "        from scipy.stats import ttest_ind_from_stats\n",
        "        import numpy as np\n",
        "        ret_dict = dict()\n",
        "        statistic, p_val = ttest_ind_from_stats(x[\"N_{0}_avg\".format(cur_label)], x[\"N_{0}_std\".format(cur_label)], x[\"N_{0}_count\".format(cur_label)], ref_avg, ref_std, ref_count, equal_var=True)\n",
        "        if not np.isnan(statistic):\n",
        "            if statistic > 0:\n",
        "                one_tailed_pval = 1.0 - p_val/2.0\n",
        "            else:\n",
        "                one_tailed_pval = p_val/2.0\n",
        "            ret_dict[\"N_{0}_TT\".format(cur_label)] = float(p_val)\n",
        "            ret_dict[\"N_{0}_LT\".format(cur_label)] = float(one_tailed_pval)\n",
        "        return ret_dict\n",
        "\n",
        "def merge_dict_all(x): #will not be used outside of the package\n",
        "        ret_dict = dict()\n",
        "        for cur_dict in x:\n",
        "            ret_dict.update(cur_dict)\n",
        "        return ret_dict\n",
        "\n",
        "def feature_aggregator(num_features,cat_features,catinputCol=\"cat_features\",numinputCol=\"num_features\",aggregatorCol = [\"ID\",\"TIME_SPAN\"], outputCol=\"feature_aggregated\",idCol=\"ID\",REPARTITION_CONST = None):\n",
        "        from pyspark.sql.functions import col, udf, collect_list,rand\n",
        "        from pyspark.sql.types import MapType, StringType, DoubleType\n",
        "\n",
        "        if not num_features:\n",
        "            ret_data_frame=cat_features.withColumnRenamed(catinputCol,\"features\")\n",
        "\n",
        "        elif not cat_features:\n",
        "            ret_data_frame=num_features.withColumnRenamed(numinputCol,\"features\")\n",
        "\n",
        "        else:\n",
        "            ret_data_frame = num_features.select(aggregatorCol+[numinputCol]).withColumnRenamed(numinputCol,\"features\")\\\n",
        "                                     .unionAll(cat_features.select(aggregatorCol+[catinputCol]).withColumnRenamed(catinputCol,\"features\"))\n",
        "\n",
        "        if REPARTITION_CONST is not None:\n",
        "            ret_data_frame = ret_data_frame.repartition(REPARTITION_CONST)\n",
        "\n",
        "        udf_merge_dict_all = udf(merge_dict_all,MapType(StringType(),DoubleType()))\n",
        "\n",
        "        # FROM HERE\n",
        "        ret_data_frame = ret_data_frame.groupBy(aggregatorCol)\n",
        "        if REPARTITION_CONST is not None:\n",
        "            ret_data_frame = ret_data_frame.agg(collect_list(\"features\").alias(\"features\")).repartition(REPARTITION_CONST)\n",
        "\n",
        "        else:\n",
        "            ret_data_frame = ret_data_frame.agg(collect_list(\"features\").alias(\"features\"))\n",
        "        #ret_data_frame.orderBy(rand()).show(truncate=False)\n",
        "        ret_data_frame = ret_data_frame.withColumn(outputCol, udf_merge_dict_all(\"features\"))\n",
        "        return ret_data_frame\n",
        "\n",
        "def flattener_df_prep(data_frame, descCol=[\"ID\",\"TIME_SPAN\"] ,inputCol=\"feature_aggregated\",drop_cnt=True):\n",
        "        from pyspark.sql import Row\n",
        "        from pyspark.sql.functions import col,udf,lit\n",
        "        from pyspark.sql.types import StructType,StructField,StringType,DoubleType,BooleanType\n",
        "        data_frame.show()\n",
        "        desc_schema = data_frame.select(descCol).schema\n",
        "        all_feature_column = list(data_frame.select(inputCol).rdd.map(lambda x: set(x[inputCol].keys())).reduce(lambda a,b: a.union(b)))\n",
        "        ret_df = data_frame.rdd.map(lambda x: x.asDict()).map(lambda cur_item: [dict((cur_col,cur_item[cur_col]) for cur_col in descCol)]\\\n",
        "                                                                                + [cur_item[inputCol]]).map(lambda x: merge_dict_all(x))\n",
        "        inst_count = data_frame.count()\n",
        "        ret_schema = desc_schema\n",
        "        ret_feature_col = list()\n",
        "        key_checker = udf(check_key_in_dict,BooleanType())\n",
        "        for cur_col in all_feature_column:\n",
        "    #        print (\"{0}//{1}//{2}\".format(data_frame.where(key_checker(inputCol,lit(cur_col))).count(),inst_count,cur_col))\n",
        "    #        if (data_frame.where(key_checker(inputCol,lit(cur_col))).count() == inst_count):\n",
        "    #            continue\n",
        "            if drop_cnt:\n",
        "                if cur_col.find(\"count\") != -1:\n",
        "                    continue\n",
        "            ret_feature_col.append(cur_col)\n",
        "            ret_schema = ret_schema.add(StructField(cur_col,DoubleType(),True))\n",
        "        return (ret_df, ret_schema,ret_feature_col)\n",
        "\n",
        "def check_key_in_dict(target_dict,target_key):\n",
        "        return target_dict.has_key(target_key)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPG1_-uWNPCi"
      },
      "source": [
        "#output deleted for readability (VERY long)\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "spark = SparkSession(SparkContext.getOrCreate())\n",
        "th_range = [0.5]\n",
        "cur_preprocessed = run_preprocessor(obs_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGzCSF9FMa4F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f14a6a0-24f5-4bdd-86e2-a9cf402f4e09"
      },
      "source": [
        "x = cur_preprocessed.toPandas()\n",
        "print(x.head())\n",
        "len(x)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         ID  ... N_220545_TT\n",
            "0  130108.0  ...    0.290789\n",
            "1  130108.0  ...    0.722685\n",
            "2  118007.0  ...    0.840266\n",
            "3  183762.0  ...    0.401749\n",
            "4  143047.0  ...    0.022046\n",
            "\n",
            "[5 rows x 350 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3104"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8nY4bz5unjq"
      },
      "source": [
        "def annotate_dataset(action_df, cur_df, terminal_outcome, annotation_method = \"p_val\", sel_top = 5):\n",
        "    '''\n",
        "    :param cur_df:\n",
        "    :param annotation_method:\n",
        "    :param postfix:\n",
        "    :param cur_top:\n",
        "    :param non_feature_column:\n",
        "    :param add_flag:\n",
        "    :return:\n",
        "    '''\n",
        "    if annotation_method == \"p_val\":\n",
        "        return annotate_pval_dataset(action_df, cur_df, terminal_outcome, sel_top = sel_top)\n",
        "\n",
        "    return \n",
        "def annotate_pval_dataset(action_df, cur_df, terminal_outcome, non_feature_column = [\"ID\", \"TIME_SPAN\"], sel_top = 5):\n",
        "    import pyspark\n",
        "    from pyspark.ml.feature import VectorAssembler\n",
        "    obs_df = cur_df\n",
        "    itemid = \"ITEMID\"\n",
        "    cur_cols = obs_df.columns\n",
        "    for i in non_feature_column:\n",
        "        cur_cols.remove(i)\n",
        "\n",
        "    cur_cols = sorted(cur_cols)\n",
        "    import json\n",
        "    #json.dump({\"non_demo_features\":cur_cols},open(self.json_feature_dump_loc,\"w\"))\n",
        "    obs_df = VectorAssembler(inputCols=cur_cols, outputCol=\"features_imputed\").transform(obs_df)\n",
        "\n",
        "    cur_time_list = obs_df.select(\"ID\",\"TIME_SPAN\")\n",
        "    of_annotated = obs_df\n",
        "    of_excl_training = dict()\n",
        "    demo_feature = add_demo()\n",
        "    #from pyspark.context import SparkContext\n",
        "    #from pyspark.sql.session import SparkSession\n",
        "    #sc = SparkContext.getOrCreate();\n",
        "    #spark = SparkSession(sc)\n",
        "    #action_df.TIME_OBS = action_df.TIME_OBS.astype(str)\n",
        "    #demo_feature = spark.createDataFrame(demo_feature)\n",
        "    \n",
        "    of_annotated = VectorAssembler(inputCols=[\"features_imputed\",\"demo_feature\"],outputCol=\"features\").transform(of_annotated.join(demo_feature,\"ID\"))\n",
        "    from pyspark.sql.functions import col,lit,when\n",
        "    cur_test_ids = get_target_test_id()\n",
        "    tr_inst,te_inst = prep_TR_TE(of_annotated,test_id_list = cur_test_ids)\n",
        "    train_data_ID = tr_inst.select(\"ID\").distinct().rdd.flatMap(list).collect()\n",
        "\n",
        "    testing_data_ID = te_inst.select(\"ID\").distinct().rdd.flatMap(list).collect()\n",
        "\n",
        "    train_action_df = action_df.where(col(\"ID\").isin(train_data_ID)).persist()\n",
        "    train_terminal_outcome = terminal_outcome.where(col(\"ID\").isin(train_data_ID)).persist()\n",
        "    intv_w_p_val = identify_relevant_action(train_action_df, train_terminal_outcome, tr_inst.select(\"ID\").distinct().count(), terminal_outcome)\n",
        "    #intv_w_p_val.join(def_df[def_df[\"SOURCE\"].isin([\"CPT\",\"MED\",\"PROC\"])], itemid).orderBy(\"p_val\").show(100, truncate=False)\n",
        "    from pyspark.sql.functions import sum,rand,max,lit\n",
        "    from pyspark.ml.feature import VectorAssembler\n",
        "    annot_df = action_df.join(terminal_outcome,\"ID\").persist()\n",
        "    pos_inst_dict = dict()\n",
        "    from pyspark.sql.functions import count\n",
        "    cur_annot_topk = sel_top\n",
        "    for cur_of in [target_disch_col]:\n",
        "        # For debug purpose, pass if target_of is not identified\n",
        "        intv_w_p_val.where(\"DISCH_DX == '{0}'\".format(cur_of)).orderBy(col(\"p_val\").cast(\"double\")).show(50,truncate=False)\n",
        "        target_annot_criteria = intv_w_p_val.where(\"DISCH_DX == '{0}'\".format(cur_of)).orderBy(col(\"p_val\").cast(\"double\")).limit(cur_annot_topk)\n",
        "       # target_annot_criteria.write.save(self.annot_intv_dir.format(cur_of,cur_annot_topk),mode=\"overwrite\")\n",
        "        target_annot_criteria = target_annot_criteria.select(itemid).rdd.flatMap(list).collect()\n",
        "        if len(target_annot_criteria) == 0:\n",
        "            pos_inst_dict[cur_of] = None\n",
        "            continue\n",
        "        print(\"checkpoint 10\")\n",
        "        #def_df[def_df[\"ITEMID\"].isin(target_annot_criteria)].show(cur_annot_topk,truncate=False)\n",
        "        print(\"checkpoint 11\")\n",
        "        pos_inst_dict[cur_of] = annot_df.where((col(itemid).isin(target_annot_criteria)) & (col(\"DISCH_DX\") == cur_of))\\\n",
        "            .select(\"ID\", col(\"TIME_OBS\").cast(\"date\").alias(\"TIME_OBS\"), lit(\"1\").cast(\"double\").alias(\"{0}_label\".format(cur_of)))\\\n",
        "            .distinct().persist()\n",
        "        print(\"checkpoint 12\")\n",
        "        #pos_inst_dict[cur_of].groupBy(\"{0}_label\".format(cur_of)).agg(count(\"*\")).show()\n",
        "        from pyspark.sql.functions import broadcast\n",
        "\n",
        "        true_inst = annot_df.where((col(itemid).isin(target_annot_criteria)) & (col(\"DISCH_DX\") == cur_of))\n",
        "        excl_id = annot_df.withColumn(\"IS_TARGET_OF\",when(col(\"DISCH_DX\") ==cur_of,lit(\"1\").cast(\"double\")).otherwise(lit(\"0\").cast(\"double\")))\\\n",
        "            .withColumn(\"IS_REL_INTV\", when(col(itemid).isin(target_annot_criteria), lit(\"1\").cast(\"double\")).otherwise(lit(\"0\").cast(\"double\")))\\\n",
        "            .groupBy(\"ID\").agg(sum(\"IS_TARGET_OF\").alias(\"SUM_IS_TARGET_OF\"),sum(\"IS_REL_INTV\").alias(\"SUM_IS_REL_INTV\"))\\\n",
        "            .where(\"(SUM_IS_TARGET_OF <> 0) AND (SUM_IS_REL_INTV == 0)\").select(\"ID\").distinct().rdd.flatMap(list).collect()\n",
        "        print(\"checkpoint 13\")\n",
        "        tr_inst = tr_inst.withColumn(\"TIME_OBS\",col(\"TIME_SPAN.TIME_TO\").cast(\"date\"))\\\n",
        "            .withColumn(\"{0}_excl\".format(cur_of), col(\"ID\").isin(excl_id).cast(\"double\")).repartition(\"ID\",\"TIME_OBS\")\\\n",
        "            .join(broadcast(pos_inst_dict[cur_of]),[\"ID\",\"TIME_OBS\"],\"left_outer\").fillna(value=0.0,subset=[\"{0}_label\".format(cur_of)]).persist()\n",
        "        print(tr_inst.count())\n",
        "        #tr_inst.groupBy(\"{0}_label\".format(cur_of),\"{0}_excl\".format(cur_of)).agg(count(\"*\")).show()\n",
        "        te_inst = te_inst.withColumn(\"TIME_OBS\",col(\"TIME_SPAN.TIME_TO\").cast(\"date\"))\\\n",
        "            .withColumn(\"{0}_excl\".format(cur_of), col(\"ID\").isin(excl_id).cast(\"double\")).repartition(\"ID\",\"TIME_OBS\")\\\n",
        "            .join(broadcast(pos_inst_dict[cur_of]),[\"ID\",\"TIME_OBS\"],\"left_outer\").fillna(value=0.0, subset=[\"{0}_label\".format(cur_of)]).persist()\n",
        "        print(te_inst.count())\n",
        "        #te_inst.groupBy(\"{0}_label\".format(cur_of),\"{0}_excl\".format(cur_of)).agg(count(\"*\")).show()\n",
        "\n",
        "        tr_inst.groupBy(\"ID\").agg(max(\"{0}_label\".format(cur_of)).alias(\"{0}_label\".format(cur_of)),max(\"{0}_excl\".format(cur_of)).alias(\"{0}_excl\".format(cur_of))).groupBy(\"{0}_label\".format(cur_of),\"{0}_excl\".format(cur_of)).agg(count(\"*\")).show()\n",
        "        te_inst.groupBy(\"ID\").agg(max(\"{0}_label\".format(cur_of)).alias(\"{0}_label\".format(cur_of)),max(\"{0}_excl\".format(cur_of)).alias(\"{0}_excl\".format(cur_of))).groupBy(\"{0}_label\".format(cur_of),\"{0}_excl\".format(cur_of)).agg(count(\"*\")).show()\n",
        "\n",
        "    #tr_inst.write.save(self.training_temp_dir, mode=\"overwrite\")\n",
        "    #te_inst.write.save(self.testing_temp_dir, mode=\"overwrite\")\n",
        "\n",
        "    #tr_inst = self.spark.read.parquet(self.training_temp_dir)\n",
        "    #te_inst = self.spark.read.parquet(self.testing_temp_dir)\n",
        "    #te_inst.show()\n",
        "    return (tr_inst,te_inst)\n",
        "\n",
        "def identify_relevant_action(action_df, terminal_df,cnt_pop, terminal_outcome):\n",
        "        from pyspark.sql.functions import col\n",
        "        def ret_p_val(obs,cnt_pop,p):\n",
        "            from scipy.stats import binom_test\n",
        "            return float(binom_test(obs,cnt_pop,p,alternative='greater'))\n",
        "\n",
        "        from pyspark.sql.functions import count, udf,col\n",
        "        from pyspark.sql.types import DoubleType\n",
        "\n",
        "        cur_dx_cnt = terminal_outcome.groupBy(\"DISCH_DX\").agg(count(\"*\").alias(\"DISCH_DX_CNT\")).cache()\n",
        "        distinct_action_df = action_df.select(\"ID\",\"ITEMID\").distinct().cache()\n",
        "        agg_action_df = distinct_action_df.groupBy(\"ITEMID\").agg(count(\"*\").alias(\"action_cnt\")).withColumn(\"action_prop\",col(\"action_cnt\")/float(cnt_pop))\n",
        "        merged_terminal_action = terminal_df.join(distinct_action_df,\"ID\").groupBy(\"DISCH_DX\",\"ITEMID\").agg(count(\"*\").alias(\"DISCH_DX_ACTION_CNT\")).join(cur_dx_cnt,\"DISCH_DX\").join(agg_action_df,\"ITEMID\").persist()\n",
        "\n",
        "        udf_binom_test = udf(ret_p_val,DoubleType())\n",
        "        cur_test = merged_terminal_action.withColumn(\"p_val\",udf_binom_test(\"DISCH_DX_ACTION_CNT\",\"DISCH_DX_CNT\",\"action_prop\")).cache()\n",
        "        return cur_test\n",
        "def prep_TR_TE(merged_df, per_instance=False, tr_prop=0.9,targetCol=\"ID\",test_id_list = []):\n",
        "    from pyspark.sql.functions import col\n",
        "    if len(test_id_list) != 0:\n",
        "        tr_inst = merged_df.where(~col(targetCol).isin(test_id_list))\n",
        "        te_inst = merged_df.where(col(targetCol).isin(test_id_list))\n",
        "        return (tr_inst,te_inst)\n",
        "    if per_instance:\n",
        "        tr_inst, te_inst = merged_df.randomSplit([tr_prop, 1-tr_prop])\n",
        "    else:\n",
        "        tr_id, te_id = merged_df.select(targetCol).distinct().randomSplit([tr_prop,1-tr_prop])\n",
        "        tr_id = tr_id.rdd.flatMap(list).collect()\n",
        "        te_id = te_id.rdd.flatMap(list).collect()\n",
        "        tr_inst = merged_df.where(col(targetCol).isin(tr_id))\n",
        "        te_inst = merged_df.where(col(targetCol).isin(te_id))\n",
        "    return (tr_inst,te_inst)\n",
        "\n",
        "def run_RF(tr_inst,te_inst, target_disch_col, eval_performance_criteria = \"AUROC\", eval_cv_or_tvt = \"CV\", model_of = [], cur_cv_fold = 2):\n",
        "        from pyspark.sql.functions import col\n",
        "\n",
        "        if model_of == []:\n",
        "            model_of = target_disch_col\n",
        "        if type(model_of) == str:\n",
        "            model_of = [model_of]\n",
        "\n",
        "        from pyspark.ml.classification import GBTClassifier as cur_model_selection\n",
        "\n",
        "        cur_classifier = cur_model_selection(featuresCol=\"features\",checkpointInterval=5)\n",
        "\n",
        "        from pyspark.ml import Pipeline\n",
        "        from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "        from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "        if eval_performance_criteria == \"AUPRC\":\n",
        "            target_metric=\"areaUnderPR\"\n",
        "        elif eval_performance_criteria == \"AUROC\":\n",
        "            target_metric=\"areaUnderROC\"\n",
        "        else:\n",
        "            raise Exception(\"eval_metric should be either 'AUPRC' or 'AUROC'\")\n",
        "\n",
        "\n",
        "        evaluator = BinaryClassificationEvaluator(metricName=target_metric)\n",
        "        paramGrid = get_param_grid(cur_model_selection) ########\n",
        "\n",
        "        if eval_cv_or_tvt == \"CV\":\n",
        "            pipeline = Pipeline(stages=[cur_classifier])\n",
        "            orig_tr_inst = tr_inst\n",
        "            orig_te_inst = te_inst\n",
        "            from pyspark.sql.functions import count, datediff\n",
        "            from pyspark.sql.functions import udf, log, sum, exp, max\n",
        "            udf_prob = udf(lambda x: x.toArray().tolist()[1])\n",
        "            from pyspark.sql.functions import corr, udf, isnan\n",
        "            for cur_of in model_of:\n",
        "                #should move this to back\n",
        "                te_inst = orig_te_inst.withColumn(\"label\", col(\"{0}_label\".format(cur_of)).cast(\"double\"))\n",
        "                te_inst.groupBy(\"label\").agg(count(\"*\")).show()\n",
        "\n",
        "                tr_inst.printSchema()\n",
        "                tr_val_pts_dict = get_target_tr_val_id() #####\n",
        "                orig_tr_inst = tr_inst\n",
        "\n",
        "                tr_pts = tr_val_pts_dict[\"TR\"]\n",
        "                val_pts = tr_val_pts_dict[\"VAL\"]\n",
        "\n",
        "                all_training_ids = tr_pts+val_pts\n",
        "                from random import shuffle\n",
        "                shuffle(all_training_ids)\n",
        "                import numpy as np\n",
        "                print(len(all_training_ids))\n",
        "                cv_id_list_full = np.array(all_training_ids)\n",
        "                perform_dict = dict()\n",
        "                for cur_cv_stage in range(cur_cv_fold):\n",
        "                    tr_pts = cv_id_list_full[np.linspace(0,cv_id_list_full.shape[0]-1,cv_id_list_full.shape[0])%cur_cv_fold != cur_cv_stage].tolist()\n",
        "                    val_pts = cv_id_list_full[np.linspace(0,cv_id_list_full.shape[0]-1,cv_id_list_full.shape[0])%cur_cv_fold == cur_cv_stage].tolist()\n",
        "                    print(np.linspace(0,cv_id_list_full.shape[0]-1,cv_id_list_full.shape[0])%cur_cv_fold == cur_cv_stage)\n",
        "                    print(cv_id_list_full[np.linspace(0,cv_id_list_full.shape[0]-1,cv_id_list_full.shape[0])%cur_cv_fold == cur_cv_stage])\n",
        "                    print(\"VAL_ROUND_{0}_TARGET IDS:{1}\".format(cur_cv_stage,val_pts))\n",
        "\n",
        "                    tr_inst = orig_tr_inst.where(col(\"ID\").isin(tr_pts))#.persist()\n",
        "                    val_inst = orig_tr_inst.where(col(\"ID\").isin(val_pts))#.persist()\n",
        "                    tr_inst = tr_inst.where(col(\"{0}_excl\".format(cur_of)) == 0).withColumn(\"label\", col(\n",
        "                        \"{0}_label\".format(cur_of)).cast(\"double\"))\n",
        "                    val_inst = val_inst.withColumn(\"label\", col(\n",
        "                        \"{0}_label\".format(cur_of)).cast(\"double\"))\n",
        "                    \n",
        "                    tr_inst.groupBy(\"label\").agg(count(\"*\")).show()\n",
        "                    pipeline_models = pipeline.fit(tr_inst, params=paramGrid)\n",
        "\n",
        "                    for cur_model in pipeline_models:\n",
        "                        val_pred = cur_model.transform(val_inst)\n",
        "                        agg_prob_val = val_pred.groupBy(\"ID\").agg(max(\"label\").alias(\"label\"),\n",
        "                                                                  sum(log(1.0 - udf_prob(\"Probability\"))).alias(\n",
        "                                                                      \"inverse_log_sum\")) \\\n",
        "                            .select(\"label\", (1.0 - exp(col(\"inverse_log_sum\"))).alias(\"rawPrediction\"))\n",
        "                        agg_prob_val.show(300, truncate=False)\n",
        "                        cur_pr = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\",\n",
        "                                                               metricName=target_metric).evaluate(agg_prob_val)\n",
        "                        if str(cur_model.stages[-1].extractParamMap()) not in perform_dict:\n",
        "                            perform_dict[str(cur_model.stages[-1].extractParamMap())] = dict()\n",
        "                            perform_dict[str(cur_model.stages[-1].extractParamMap())][\"PERF\"]=list()\n",
        "                            perform_dict[str(cur_model.stages[-1].extractParamMap())][\"PARAM\"] = cur_model.stages[-1].extractParamMap()\n",
        "\n",
        "                        perform_dict[str(cur_model.stages[-1].extractParamMap())][\"PERF\"].append(cur_pr)\n",
        "\n",
        "                best_pf_measure = -1\n",
        "                best_pf_param = None\n",
        "\n",
        "                for key in perform_dict:\n",
        "                    import numpy as np\n",
        "                    test_array = np.array(perform_dict[key][\"PERF\"])\n",
        "                    print(key, test_array.mean(), test_array.std())\n",
        "                    if best_pf_measure < test_array.mean():\n",
        "                        best_pf_measure = test_array.mean()\n",
        "                        best_pf_param = perform_dict[key][\"PARAM\"]\n",
        "\n",
        "\n",
        "                print(\"retrain model based on best hp from CV\")\n",
        "                print(\"PERF:{0}\".format(best_pf_measure))\n",
        "                print(\"HP:{0}\".format(best_pf_param))\n",
        "                tr_inst = orig_tr_inst.where(col(\"{0}_excl\".format(cur_of)) == 0).withColumn(\"label\", col(\n",
        "                    \"{0}_label\".format(cur_of)).cast(\"double\"))\n",
        "\n",
        "                bestModel = pipeline.fit(tr_inst.where(col(\"ID\").isin(cv_id_list_full.tolist())),params=[best_pf_param])[0]\n",
        "\n",
        "                #bestModel.save(self.model_dir_template.format(cur_of, best_pf_measure))\n",
        "\n",
        "                prediction = bestModel.transform(te_inst)\n",
        "                prediction.show()\n",
        "               # prediction.write.save(self.testing_result_dest_template.format(cur_of), mode=\"overwrite\")\n",
        "                tr_result = bestModel.transform(tr_inst).withColumn(\"Prob\",udf_prob(\"Probability\"))\n",
        "                #tr_result.write.save(self.training_result_dest_template.format(cur_of), mode=\"overwrite\")\n",
        "                return prediction, tr_result\n",
        "\n",
        "def get_param_grid(cur_model_selection):\n",
        "        from pyspark.ml.tuning import ParamGridBuilder\n",
        "        return ParamGridBuilder() \\\n",
        "            .addGrid(cur_model_selection.maxDepth, [2]) \\\n",
        "            .addGrid(cur_model_selection.subsamplingRate, [0.3,0.8]) \\\n",
        "            .addGrid(cur_model_selection.maxIter, [2]) \\\n",
        "            .build()\n",
        "def handle_missing(cur_df, non_feature_col = [\"ID\", \"TIME_SPAN\"]):\n",
        "      import pyspark\n",
        "      cur_cols = cur_df.columns\n",
        "      categorical_cols = list()\n",
        "      numerical_cols = list()\n",
        "      for i in non_feature_col:\n",
        "          cur_cols.remove(i)\n",
        "      for i in cur_cols:\n",
        "          if i.find(\"C_\") == 0:\n",
        "              categorical_cols.append(i)\n",
        "          else:\n",
        "              numerical_cols.append(i)\n",
        "\n",
        "      cur_df = cur_df.fillna(0,subset = categorical_cols).repartition(400)\n",
        "\n",
        "      from pyspark.ml.feature import Imputer\n",
        "      imputedCols = [\"imp_{0}\".format(x) for x in numerical_cols]\n",
        "      imputer = Imputer(inputCols = numerical_cols,outputCols = imputedCols).setStrategy(\"mean\")\n",
        "      imputer_model = imputer.fit(cur_df)\n",
        "      ret_data_frame = imputer_model.transform(cur_df)\n",
        "      #ret_data_frame.select(non_feature_col+imputedCols+categorical_cols).show()\n",
        "      #ret_data_frame.select(non_feature_col+imputedCols+categorical_cols).write.save(self.temp_missing_drop)\n",
        "      #ret_data_frame = self.spark.read.parquet(self.temp_missing_drop)\n",
        "      return ret_data_frame.select(non_feature_col+imputedCols+categorical_cols)\n",
        "def define_and_normalize_terminal_df(action_df, def_df):\n",
        "      '''\n",
        "      exclude action df that doesn't fit to current criteria\n",
        "      :return:\n",
        "      '''\n",
        "      from pyspark.context import SparkContext\n",
        "      from pyspark.sql.session import SparkSession\n",
        "      sc = SparkContext.getOrCreate();\n",
        "      spark = SparkSession(sc)\n",
        "      action_df.TIME_OBS = action_df.TIME_OBS.astype(str)\n",
        "      action_df = spark.createDataFrame(action_df)\n",
        "\n",
        "      cur_action_col = action_df\n",
        "      cur_def = def_df\n",
        "\n",
        "      original_def = cur_def\n",
        "      all_itemid =get_action_itemids()\n",
        "      return action_df.join(all_itemid,[\"ITEMID\",\"SOURCE\"]).persist()\n",
        "\n",
        "def flatten_terminal_outcome(action_df, cur_terminal_df, target_disch_col, target_disch_icd9):\n",
        "      from pyspark.sql.functions import max,col,lit\n",
        "      from pyspark.context import SparkContext\n",
        "      from pyspark.sql.session import SparkSession\n",
        "      sc = SparkContext.getOrCreate();\n",
        "      spark = SparkSession(sc)\n",
        "      action_df.TIME_OBS = action_df.TIME_OBS.astype(str)\n",
        "      action_df = spark.createDataFrame(action_df)\n",
        "      \n",
        "      terminal_action = action_df.select(\"ID\", \"ITEMID\").distinct()\n",
        "      cur_terminal_df = spark.createDataFrame(cur_terminal_df)\n",
        "      terminal_outcome = cur_terminal_df\n",
        "      from pyspark.sql.functions import when\n",
        "      cur_of_col = [target_disch_col]\n",
        "      terminal_outcome = terminal_outcome.withColumn(target_disch_col,when(col(\"ICD9_CODE\").isin(target_disch_icd9),lit(\"1\")).otherwise(lit(\"0\")))\\\n",
        "          .select([\"ID\"] + cur_of_col).groupBy(\"ID\").agg(\n",
        "          *(max(c).alias(c) for c in cur_of_col)).select(\n",
        "          *(col(c).cast(\"double\").alias(c) if c in cur_of_col else col(c) for c in [\"ID\"] + cur_of_col))\n",
        "      raw_terminal_outcome = terminal_outcome\n",
        "      from pyspark.sql.functions import rand\n",
        "      #cms_dx_def = self.cur_annotator.annotate_of_label()\n",
        "\n",
        "      target_terminal_outcome_table = terminal_outcome\n",
        "      flatten_of_list = list()\n",
        "      for cur_of in [target_disch_col]:\n",
        "          flatten_of_list.append(terminal_outcome.where(\"{0} == 1\".format(cur_of)).select(\"ID\",lit(\"{0}\".format(cur_of)).alias(\"DISCH_DX\")))\n",
        "\n",
        "\n",
        "\n",
        "      from functools import reduce as f_reduce\n",
        "      from pyspark.sql import DataFrame\n",
        "      terminal_outcome = f_reduce(DataFrame.union, flatten_of_list).persist()\n",
        "      return terminal_outcome\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_pL-uZvL1kc"
      },
      "source": [
        "#work on run_experiment\n",
        "def run_experiment(action_df, cur_terminal_df, target_disch_col, target_disch_icd9, cur_preprocesssed, num_intv = 5):\n",
        "        '''\n",
        "        print agg_prob, corr_predicted_risks and return following\n",
        "        :return: dict tr_instance df, dict te_instance df, dict model(CVModel)\n",
        "        '''\n",
        "        from pyspark.ml import PipelineModel\n",
        "        import pyspark\n",
        "        #self.set_top_intv_k(num_intv) -> does nothing - >Edit NEED TO INCLUDE FOR MULTIPLE INTERVENTIONS in the annotate_dataset method\n",
        "        sel_top = num_intv\n",
        "        terminal_outcome = flatten_terminal_outcome(action_df, cur_terminal_df, target_disch_col, target_disch_icd9)\n",
        "        from glob import glob\n",
        "        tr_result = dict()\n",
        "        te_result = dict()\n",
        "        ret_model = dict()\n",
        "        for cur_of in [target_disch_col]:\n",
        "            def_df = get_def_df()\n",
        "            action_df = define_and_normalize_terminal_df(action_df, def_df)\n",
        "            cur_df = cur_preprocessed \n",
        "            preprocessed_data = handle_missing(cur_df, non_feature_col = [\"ID\", \"TIME_SPAN\"]) \n",
        "            tr_inst, te_inst = annotate_dataset(action_df, preprocessed_data, terminal_outcome, sel_top = sel_top)\n",
        "            tr_inst.show()\n",
        "            te_inst.show()\n",
        "            return run_RF(tr_inst, te_inst, target_disch_col, model_of = cur_of)\n",
        "\n",
        "            #cur_model = glob(self.model_dir_template.format(cur_of,\"*\"))\n",
        "            #print(cur_model)\n",
        "\n",
        "            #cur_model = cur_model[0]\n",
        "            #ret_model[cur_of] = PipelineModel.load(cur_model)\n",
        "\n",
        "            #tr_result[cur_of] = self.spark.read.parquet(self.training_result_dest_template.format(cur_of))\n",
        "            #te_result[cur_of] = self.spark.read.parquet(self.testing_result_dest_template.format(cur_of))\n",
        "\n",
        "        return tr_result, te_result#, cur_model"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot1GsDmWgfQI"
      },
      "source": [
        "target_disch = [\"51881\", \"51851\", \"51884\", \"51853\"]\n",
        "target_disch_col = \"DISCH_{0}\".format(\"_\".join(target_disch))\n",
        "target_disch_icd9 = [\"51881\", \"51851\", \"51884\", \"51853\"]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nV-0jTiPRbH"
      },
      "source": [
        "#tr_result and te_result are backwards\n",
        "#output deleted for readability (VERY long)\n",
        "for cur_intv_num in [10]:\n",
        "  tr_result, te_result = run_experiment(action_df, terminal_df, target_disch_col, target_disch_icd9, cur_preprocessed, num_intv = cur_intv_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZufcacCJUxv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4409a35b-daaa-4455-a6df-e9b703b9dede"
      },
      "source": [
        "#training and testing raw accuracy from Dae's code (GBT w/ 5-fold CV)\n",
        "#tr_resultdf became te_result to corrrect for backwards return, can verify by checking the length\n",
        "tr_resultdf = te_result.toPandas()\n",
        "cor = 0\n",
        "for i in range(len(tr_resultdf)):\n",
        "  if(tr_resultdf[\"prediction\"][i] == tr_resultdf[\"label\"][i]):\n",
        "    cor += 1\n",
        "print(\"training accuracy :\" + str(cor / len(tr_resultdf)))\n",
        "te_resultdf = tr_result.toPandas()\n",
        "cor = 0\n",
        "for i in range(len(te_resultdf)):\n",
        "  if(te_resultdf[\"prediction\"][i] == te_resultdf[\"label\"][i]):\n",
        "    cor += 1\n",
        "print(\"testing accuracy: \" + str(cor / len(te_resultdf)))\n",
        "print(\"len training: \" + str(len(tr_resultdf)))\n",
        "print(\"len testing: \" + str(len(te_resultdf)))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training accuracy :0.9351214208046393\n",
            "testing accuracy: 0.735202492211838\n",
            "len training: 2759\n",
            "len testing: 321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwHf8IZyq8iw"
      },
      "source": [
        "from pyspark.ml.linalg import DenseVector\n",
        "#double check whether to take from label column name or the other one -> perhaps try both and see which is better (that should be the combined one), the .map is to uncondense Dense Vectorss\n",
        "X_train = tr_resultdf[\"features_imputed\"]\n",
        "X_test = te_resultdf[\"features_imputed\"]\n",
        "y_train = tr_resultdf[\"label\"]\n",
        "y_test = te_resultdf[\"label\"]\n",
        "X_train = X_train.map(lambda vector: vector.toArray())\n",
        "X_test = X_test.map(lambda vector: vector.toArray())"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bSfoOQMuiEj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68ec6cd3-8c24-4de8-d035-38907d10b3be"
      },
      "source": [
        "#vstack to increase shape for sklearn / tf\n",
        "#double check if training set split is supposed to be this large (closer to 90 rather than 70-80)\n",
        "import numpy as np\n",
        "X_train = np.vstack(X_train)\n",
        "X_test = np.vstack(X_test)\n",
        "print(X_train.shape)\n",
        "X_test.shape"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2759, 348)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(321, 348)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUhw_FO6Lvtp"
      },
      "source": [
        "def print_counts(y_actual, y_hat):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "    for i in range(len(y_hat)): \n",
        "        if y_actual[i]==y_hat[i]==1:\n",
        "           TP += 1\n",
        "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
        "           FP += 1\n",
        "        if y_actual[i]==y_hat[i]==0:\n",
        "           TN += 1\n",
        "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
        "           FN += 1\n",
        "\n",
        "    print(\"true positive count: \" + str(TP))\n",
        "    print(\"false positive count: \" + str(FP))\n",
        "    print(\"true negative count: \" + str(TN))\n",
        "    print(\"false negative count: \" + str(FN))\n",
        "    if(TP + FP == 0):\n",
        "      precision = 1\n",
        "    else:\n",
        "      precision = TP/ (TP + FP)\n",
        "    recall = TP / (TP + FN)\n",
        "    print(\"precision = \" + str(precision))\n",
        "    print(\"recall = \" + str(recall))\n",
        "    print(\"F1 = \" + str(2*((precision * recall) / (precision + recall))))"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SG6ToPF06Jf"
      },
      "source": [
        "## Attempting Machine Learning and Deep Learning Models W/O Class Weights or Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lbHO81srCi5"
      },
      "source": [
        "#output deleted for readability\n",
        "import sklearn\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "model = LogisticRegressionCV(cv = 10)\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score \n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == y_test[i]):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_test)))\n",
        "print_counts(y_test, preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9XhOwe13ap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af5b3fd0-2a80-4825-aed8-4d802e1905af"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.svm import SVC\n",
        "model = SVC()\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == y_test[i]):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_test)))\n",
        "print_counts(y_test, preds)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw acc 0.778816199376947\n",
            "true positive count: 0\n",
            "false positive count: 0\n",
            "true negative count: 250\n",
            "false negative count: 71\n",
            "precision = 1\n",
            "recall = 0.0\n",
            "F1 = 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SAZn5B12LgT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98fe9747-dcca-4e90-ab66-7f554f42918a"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "model = KNeighborsClassifier(n_neighbors= 5)\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == y_test[i]):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_test)))\n",
        "print_counts(y_test, preds)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw acc 0.7165109034267912\n",
            "true positive count: 14\n",
            "false positive count: 34\n",
            "true negative count: 216\n",
            "false negative count: 57\n",
            "precision = 0.2916666666666667\n",
            "recall = 0.19718309859154928\n",
            "F1 = 0.2352941176470588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsfk6Qth2eIR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "4f38ac33-09c9-4ed8-c8ec-595840fd9ceb"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "model = GaussianProcessClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == y_test[i]):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_test)))\n",
        "print_counts(y_test, preds)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw acc 0.7632398753894081\n",
            "true positive count: 0\n",
            "false positive count: 5\n",
            "true negative count: 245\n",
            "false negative count: 71\n",
            "precision = 0.0\n",
            "recall = 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-ce6e097c0ade>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raw acc \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-55-46d82d00c596>\u001b[0m in \u001b[0;36mprint_counts\u001b[0;34m(y_actual, y_hat)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"precision = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recall = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1 = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKEL3yAo1xzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a75b3ace-703f-4846-e715-8f675d632483"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == y_test[i]):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_test)))\n",
        "print_counts(y_test, preds)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw acc 0.6479750778816199\n",
            "true positive count: 20\n",
            "false positive count: 62\n",
            "true negative count: 188\n",
            "false negative count: 51\n",
            "precision = 0.24390243902439024\n",
            "recall = 0.28169014084507044\n",
            "F1 = 0.261437908496732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QOCWD4J45vi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f62c69c-6a0d-41f5-ccbc-40e620639152"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "model = GradientBoostingClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "from sklearn.metrics import f1_score\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == y_test[i]):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_test)))\n",
        "print_counts(y_test, preds)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw acc 0.7320872274143302\n",
            "true positive count: 15\n",
            "false positive count: 30\n",
            "true negative count: 220\n",
            "false negative count: 56\n",
            "precision = 0.3333333333333333\n",
            "recall = 0.2112676056338028\n",
            "F1 = 0.2586206896551724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EsgwVqtz426",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdaf88e0-44b4-440e-cc1b-a70ba649c244"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"precision score \" + str(precision_score(preds, y_test, average='macro')))\n",
        "print(\"recall score \" + str(recall_score(preds, y_test, average='macro')))\n",
        "print(\"f1 score \" + str(f1_score(preds, y_test, average='macro')))\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == y_test[i]):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_test)))\n",
        "print_counts(y_test, preds)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision score 0.48904225352112674\n",
            "recall score 0.437459807073955\n",
            "f1 score 0.4419356968376576\n",
            "raw acc 0.7538940809968847\n",
            "true positive count: 1\n",
            "false positive count: 9\n",
            "true negative count: 241\n",
            "false negative count: 70\n",
            "precision = 0.1\n",
            "recall = 0.014084507042253521\n",
            "F1 = 0.02469135802469136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXAtIKnuFG2z"
      },
      "source": [
        "y_trainencoded = [[] for i in range(len(y_train))] \n",
        "y_testencoded = [[] for i in range(len(y_test))] \n",
        "count = 0\n",
        "for element in y_train:\n",
        "  if(element == 1):\n",
        "    y_trainencoded[count].append(1)\n",
        "    y_trainencoded[count].append(0)\n",
        "  else:\n",
        "    y_trainencoded[count].append(0)\n",
        "    y_trainencoded[count].append(1)\n",
        "  count += 1\n",
        "\n",
        "count2 = 0\n",
        "for element in y_test:\n",
        "  if(element == 1):\n",
        "    y_testencoded[count2].append(1)\n",
        "    y_testencoded[count2].append(0)\n",
        "  else:\n",
        "    y_testencoded[count2].append(0)\n",
        "    y_testencoded[count2].append(1)\n",
        "  count2 += 1\n",
        "y_trainencoded = np.array(y_trainencoded)\n",
        "y_testencoded = np.array(y_testencoded)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEB5yJIK-Xnp"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, LSTM\n",
        "model = Sequential()\n",
        "model.add(Flatten())\n",
        "model.add(Dense(500, activation = 'relu')) \n",
        "model.add(Dropout(.2))\n",
        "model.add(Dense(50, activation = 'relu')) \n",
        "model.add(Dropout(.2))\n",
        "model.add(Dense(2, activation = 'softmax'))"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9hGB7r70ROx"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=\"Adam\",\n",
        "              metrics=['accuracy'])\n",
        "model.fit(X_train, y_trainencoded, batch_size = 64, epochs = 10, validation_split= .2, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtksM0gFGTeg"
      },
      "source": [
        "modelOutput = model.predict(X_test)\n",
        "preds = []\n",
        "for element in modelOutput:\n",
        "  preds.append(np.argmax(element))\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"precision score \" + str(precision_score(preds, y_test, average='macro')))\n",
        "print(\"recall score \" + str(recall_score(preds, y_test, average='macro')))\n",
        "print(\"f1 score \" + str(f1_score(preds, y_test, average='macro')))\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == np.argmax(y_testencoded[i])):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_testencoded)))\n",
        "tempPreds = []\n",
        "for i in preds:\n",
        "  tempPreds.append(np.argmax(i))\n",
        "print_counts(y_test, tempPreds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FRpAp5LHcFz"
      },
      "source": [
        "## Attempting Machine Learning and Deep Learning Models w/ Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu8SIZHGHbcA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea5d7b0d-2400-4559-e291-65cb29296b53"
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "d_class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
        "d_class_weights = {i : d_class_weights[i] for i in range(2)}\n",
        "d_class_weights"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.6404363974001857, 1: 2.2801652892561983}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO6BOoPKVwPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b6bdf0c-8014-4b06-c2a5-1fe5fd39ca4e"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(class_weight= d_class_weights)\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == y_test[i]):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_test)))\n",
        "print_counts(y_test, preds)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw acc 0.5171339563862928\n",
            "true positive count: 42\n",
            "false positive count: 126\n",
            "true negative count: 124\n",
            "false negative count: 29\n",
            "precision = 0.25\n",
            "recall = 0.5915492957746479\n",
            "F1 = 0.3514644351464435\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G66ZP_ZIWS_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afaa463a-b134-4c41-a2fc-0e897c85a62f"
      },
      "source": [
        "import sklearn\n",
        "model = SVC(class_weight= d_class_weights)\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == y_test[i]):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_test)))\n",
        "print_counts(y_test, preds)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw acc 0.5856697819314641\n",
            "true positive count: 38\n",
            "false positive count: 100\n",
            "true negative count: 150\n",
            "false negative count: 33\n",
            "precision = 0.2753623188405797\n",
            "recall = 0.5352112676056338\n",
            "F1 = 0.3636363636363636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1auP5GnJKYMd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18b95e1-5408-4299-fbc3-604a5172a514"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier(class_weight= d_class_weights)\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == y_test[i]):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_test)))\n",
        "print_counts(y_test, preds)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw acc 0.6853582554517134\n",
            "true positive count: 10\n",
            "false positive count: 40\n",
            "true negative count: 210\n",
            "false negative count: 61\n",
            "precision = 0.2\n",
            "recall = 0.14084507042253522\n",
            "F1 = 0.16528925619834714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8oM2NxsJ3QI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59830414-d435-4108-e44d-9f95e04ecfa1"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(class_weight= d_class_weights)\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == y_test[i]):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_test)))\n",
        "print_counts(y_test, preds)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw acc 0.7507788161993769\n",
            "true positive count: 1\n",
            "false positive count: 10\n",
            "true negative count: 240\n",
            "false negative count: 70\n",
            "precision = 0.09090909090909091\n",
            "recall = 0.014084507042253521\n",
            "F1 = 0.02439024390243903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2SzESGJHZjg"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, LSTM\n",
        "model = Sequential()\n",
        "model.add(Flatten())\n",
        "model.add(Dense(5000, activation = 'relu')) \n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(50, activation = 'relu')) \n",
        "model.add(Dense(2, activation = 'softmax'))"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkTxWxxGH3p9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdba4f2c-c581-4831-be9e-106a15f470d6"
      },
      "source": [
        "import keras.optimizers\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=\"Adam\",\n",
        "              metrics=['accuracy'])\n",
        "model.fit(X_train, y_trainencoded, batch_size = 64, epochs = 20, validation_split= .2, shuffle = True, class_weight = d_class_weights)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "35/35 [==============================] - 1s 9ms/step - loss: 1.2821 - accuracy: 0.7775 - val_loss: 0.6718 - val_accuracy: 0.7935\n",
            "Epoch 2/20\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.2550 - accuracy: 0.7775 - val_loss: 0.6625 - val_accuracy: 0.7935\n",
            "Epoch 3/20\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.2288 - accuracy: 0.7775 - val_loss: 0.6536 - val_accuracy: 0.7935\n",
            "Epoch 4/20\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.2033 - accuracy: 0.7775 - val_loss: 0.6451 - val_accuracy: 0.7935\n",
            "Epoch 5/20\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.1789 - accuracy: 0.7775 - val_loss: 0.6368 - val_accuracy: 0.7935\n",
            "Epoch 6/20\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.1551 - accuracy: 0.7775 - val_loss: 0.6290 - val_accuracy: 0.7935\n",
            "Epoch 7/20\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.1322 - accuracy: 0.7775 - val_loss: 0.6216 - val_accuracy: 0.7935\n",
            "Epoch 8/20\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.1099 - accuracy: 0.7775 - val_loss: 0.6144 - val_accuracy: 0.7935\n",
            "Epoch 9/20\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.0885 - accuracy: 0.7775 - val_loss: 0.6076 - val_accuracy: 0.7935\n",
            "Epoch 10/20\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.0677 - accuracy: 0.7775 - val_loss: 0.6011 - val_accuracy: 0.7935\n",
            "Epoch 11/20\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.0476 - accuracy: 0.7775 - val_loss: 0.5949 - val_accuracy: 0.7935\n",
            "Epoch 12/20\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.0281 - accuracy: 0.7775 - val_loss: 0.5891 - val_accuracy: 0.7935\n",
            "Epoch 13/20\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.0094 - accuracy: 0.7775 - val_loss: 0.5834 - val_accuracy: 0.7935\n",
            "Epoch 14/20\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.9912 - accuracy: 0.7775 - val_loss: 0.5782 - val_accuracy: 0.7935\n",
            "Epoch 15/20\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.9737 - accuracy: 0.7775 - val_loss: 0.5731 - val_accuracy: 0.7935\n",
            "Epoch 16/20\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.9568 - accuracy: 0.7775 - val_loss: 0.5684 - val_accuracy: 0.7935\n",
            "Epoch 17/20\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.9405 - accuracy: 0.7775 - val_loss: 0.5638 - val_accuracy: 0.7935\n",
            "Epoch 18/20\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.9247 - accuracy: 0.7775 - val_loss: 0.5596 - val_accuracy: 0.7935\n",
            "Epoch 19/20\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.9096 - accuracy: 0.7775 - val_loss: 0.5555 - val_accuracy: 0.7935\n",
            "Epoch 20/20\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.8948 - accuracy: 0.7775 - val_loss: 0.5517 - val_accuracy: 0.7935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f27d7ae30d0>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19cWc801IHWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c8fcc93-0997-4d8a-c1b1-c78ce209dc76"
      },
      "source": [
        "modelOutput = model.predict(X_test)\n",
        "preds = []\n",
        "for element in modelOutput:\n",
        "  preds.append(np.argmax(element))\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"precision score \" + str(precision_score(preds, y_test, average='macro')))\n",
        "print(\"recall score \" + str(recall_score(preds, y_test, average='macro')))\n",
        "print(\"f1 score \" + str(f1_score(preds, y_test, average='macro')))\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == np.argmax(y_testencoded[i])):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_testencoded)))\n",
        "tempPreds = []\n",
        "for i in preds:\n",
        "  tempPreds.append(np.argmax(i))\n",
        "print_counts(y_test, tempPreds)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision score 0.5\n",
            "recall score 0.11059190031152648\n",
            "f1 score 0.18112244897959184\n",
            "raw acc 0.778816199376947\n",
            "true positive count: 0\n",
            "false positive count: 0\n",
            "true negative count: 250\n",
            "false negative count: 71\n",
            "precision = 1\n",
            "recall = 0.0\n",
            "F1 = 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YVkWPdPMCk8"
      },
      "source": [
        "## Attempting Machine Learning and Deep Learning Models w/ Simple Random Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSfXEQ4GM2Vc"
      },
      "source": [
        "X_train = tr_resultdf[\"features_imputed\"]\n",
        "X_test = te_resultdf[\"features_imputed\"]\n",
        "y_train = tr_resultdf[\"label\"]\n",
        "y_test = te_resultdf[\"label\"]\n",
        "X_train = X_train.map(lambda vector: vector.toArray())\n",
        "X_test = X_test.map(lambda vector: vector.toArray())\n",
        "#vstack to increase shape for sklearn / tf\n",
        "X_train = np.vstack(X_train)\n",
        "X_test = np.vstack(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IseeHSp6MFCn"
      },
      "source": [
        "import imblearn\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n",
        "oversample = RandomOverSampler(sampling_strategy='minority')\n",
        "X_trainoversampled, y_trainoversampled = oversample.fit_resample(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvXjAPeqbTyu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb068f05-75e8-42d1-8857-7e42649a6c49"
      },
      "source": [
        "import sklearn\n",
        "model = LogisticRegression()\n",
        "model.fit(X_trainoversampled, y_trainoversampled)\n",
        "preds = model.predict(X_test)\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == y_test[i]):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_test)))\n",
        "print_counts(y_test, preds)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw acc 0.5700934579439252\n",
            "true positive count: 40\n",
            "false positive count: 107\n",
            "true negative count: 143\n",
            "false negative count: 31\n",
            "precision = 0.272108843537415\n",
            "recall = 0.5633802816901409\n",
            "F1 = 0.3669724770642202\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrlnVE3OMaGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4010e134-a2d6-4ea0-da7d-e30bc2357ca7"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.svm import SVC\n",
        "model = SVC()\n",
        "model.fit(X_trainoversampled, y_trainoversampled)\n",
        "preds = model.predict(X_test)\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == y_test[i]):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_test)))\n",
        "print_counts(y_test, preds)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw acc 0.5887850467289719\n",
            "true positive count: 40\n",
            "false positive count: 101\n",
            "true negative count: 149\n",
            "false negative count: 31\n",
            "precision = 0.28368794326241137\n",
            "recall = 0.5633802816901409\n",
            "F1 = 0.37735849056603776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loAVPMwTb31W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c4fae89-9bb0-49ed-b2d3-4d8b4666947a"
      },
      "source": [
        "import sklearn\n",
        "model = KNeighborsClassifier()\n",
        "model.fit(X_trainoversampled, y_trainoversampled)\n",
        "preds = model.predict(X_test)\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == y_test[i]):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_test)))\n",
        "print_counts(y_test, preds)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw acc 0.5545171339563862\n",
            "true positive count: 37\n",
            "false positive count: 109\n",
            "true negative count: 141\n",
            "false negative count: 34\n",
            "precision = 0.2534246575342466\n",
            "recall = 0.5211267605633803\n",
            "F1 = 0.34101382488479265\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68qOhiqvcJyv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "8b21d685-e162-4fdc-adba-8535d8074c94"
      },
      "source": [
        "import sklearn\n",
        "model = GaussianProcessClassifier()\n",
        "model.fit(X_trainoversampled, y_trainoversampled)\n",
        "preds = model.predict(X_test)\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == y_test[i]):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_test)))\n",
        "print_counts(y_test, preds)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw acc 0.7632398753894081\n",
            "true positive count: 0\n",
            "false positive count: 5\n",
            "true negative count: 245\n",
            "false negative count: 71\n",
            "precision = 0.0\n",
            "recall = 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-151d20ceaa93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raw acc \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-55-46d82d00c596>\u001b[0m in \u001b[0;36mprint_counts\u001b[0;34m(y_actual, y_hat)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"precision = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recall = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1 = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03f8Sj8QNfcS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3715f4e-1783-4ea1-83b8-4c0d29576e57"
      },
      "source": [
        "import sklearn\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_trainoversampled, y_trainoversampled)\n",
        "preds = model.predict(X_test)\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == y_test[i]):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_test)))\n",
        "print_counts(y_test, preds)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw acc 0.6822429906542056\n",
            "true positive count: 24\n",
            "false positive count: 55\n",
            "true negative count: 195\n",
            "false negative count: 47\n",
            "precision = 0.3037974683544304\n",
            "recall = 0.3380281690140845\n",
            "F1 = 0.31999999999999995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9Arf5G6cug4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7df5d1-dee4-41ad-cff9-638292c69864"
      },
      "source": [
        "import sklearn\n",
        "model = GradientBoostingClassifier()\n",
        "model.fit(X_trainoversampled, y_trainoversampled)\n",
        "preds = model.predict(X_test)\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == y_test[i]):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_test)))\n",
        "print_counts(y_test, preds)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw acc 0.6230529595015576\n",
            "true positive count: 27\n",
            "false positive count: 77\n",
            "true negative count: 173\n",
            "false negative count: 44\n",
            "precision = 0.25961538461538464\n",
            "recall = 0.38028169014084506\n",
            "F1 = 0.3085714285714286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw-62wLfMa2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1b2ae2-0ba0-4314-822b-bbaeb9488932"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_trainoversampled, y_trainoversampled)\n",
        "preds = model.predict(X_test)\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == y_test[i]):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_test)))\n",
        "print_counts(y_test, preds)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw acc 0.7414330218068536\n",
            "true positive count: 5\n",
            "false positive count: 17\n",
            "true negative count: 233\n",
            "false negative count: 66\n",
            "precision = 0.22727272727272727\n",
            "recall = 0.07042253521126761\n",
            "F1 = 0.10752688172043011\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jb_cDEPOUDO"
      },
      "source": [
        "y_trainencodedoversampled = [[] for i in range(len(y_trainoversampled))] \n",
        "y_testencoded = [[] for i in range(len(y_test))] \n",
        "count = 0\n",
        "for element in y_trainoversampled:\n",
        "  if(element == 1):\n",
        "    y_trainencodedoversampled[count].append(1)\n",
        "    y_trainencodedoversampled[count].append(0)\n",
        "  else:\n",
        "    y_trainencodedoversampled[count].append(0)\n",
        "    y_trainencodedoversampled[count].append(1)\n",
        "  count += 1\n",
        "\n",
        "count2 = 0\n",
        "for element in y_test:\n",
        "  if(element == 1):\n",
        "    y_testencoded[count2].append(1)\n",
        "    y_testencoded[count2].append(0)\n",
        "  else:\n",
        "    y_testencoded[count2].append(0)\n",
        "    y_testencoded[count2].append(1)\n",
        "  count2 += 1\n",
        "y_trainencodedoversampled = np.array(y_trainencodedoversampled)\n",
        "y_testencoded = np.array(y_testencoded)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncM9a8iSOFWX"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, LSTM\n",
        "model = Sequential()\n",
        "model.add(Flatten())\n",
        "model.add(Dense(500, activation = 'relu')) \n",
        "model.add(Dense(50, activation = 'relu')) \n",
        "model.add(Dense(2, activation = 'softmax'))"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3Fl7wqLOGEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e31703f5-0baf-414a-af54-c7c1330b2e40"
      },
      "source": [
        "import keras.optimizers\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=\"Adam\",\n",
        "              metrics=['accuracy'])\n",
        "model.fit(X_trainoversampled, y_trainencodedoversampled , batch_size = 64, epochs = 20, shuffle = True)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "68/68 [==============================] - 1s 3ms/step - loss: 2.8572 - accuracy: 0.5571\n",
            "Epoch 2/20\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.7188 - accuracy: 0.6144\n",
            "Epoch 3/20\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.8049 - accuracy: 0.5861\n",
            "Epoch 4/20\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.7089 - accuracy: 0.6202\n",
            "Epoch 5/20\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.6313 - accuracy: 0.6764\n",
            "Epoch 6/20\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.6624 - accuracy: 0.6551\n",
            "Epoch 7/20\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.6750 - accuracy: 0.6486\n",
            "Epoch 8/20\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.6140 - accuracy: 0.6968\n",
            "Epoch 9/20\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.5867 - accuracy: 0.7073\n",
            "Epoch 10/20\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.6344 - accuracy: 0.6869\n",
            "Epoch 11/20\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.6213 - accuracy: 0.6910\n",
            "Epoch 12/20\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.6035 - accuracy: 0.7101\n",
            "Epoch 13/20\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.6088 - accuracy: 0.6938\n",
            "Epoch 14/20\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.5876 - accuracy: 0.7173\n",
            "Epoch 15/20\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.6088 - accuracy: 0.6852\n",
            "Epoch 16/20\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.6306 - accuracy: 0.6968\n",
            "Epoch 17/20\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.6789 - accuracy: 0.5501\n",
            "Epoch 18/20\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.6935 - accuracy: 0.5000\n",
            "Epoch 19/20\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.6934 - accuracy: 0.5000\n",
            "Epoch 20/20\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.6934 - accuracy: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f27306525d0>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1qqtdCHOKRv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b9de3e-fb41-4655-ada2-337de6a97ba1"
      },
      "source": [
        "modelOutput = model.predict(X_test)\n",
        "preds = []\n",
        "for element in modelOutput:\n",
        "  preds.append(np.argmax(element))\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"precision score \" + str(precision_score(preds, y_test, average='macro')))\n",
        "print(\"recall score \" + str(recall_score(preds, y_test, average='macro')))\n",
        "print(\"f1 score \" + str(f1_score(preds, y_test, average='macro')))\n",
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  if(preds[i] == np.argmax(y_testencoded[i])):\n",
        "    count += 1\n",
        "print(\"raw acc \" + str(count / len(y_testencoded)))\n",
        "tempPreds = []\n",
        "for i in preds:\n",
        "  tempPreds.append(np.argmax(i))\n",
        "print_counts(y_test, tempPreds)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision score 0.5\n",
            "recall score 0.11059190031152648\n",
            "f1 score 0.18112244897959184\n",
            "raw acc 0.778816199376947\n",
            "true positive count: 0\n",
            "false positive count: 0\n",
            "true negative count: 250\n",
            "false negative count: 71\n",
            "precision = 1\n",
            "recall = 0.0\n",
            "F1 = 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFr11rYKZL8f"
      },
      "source": [
        "## Preprocessing Clinical Note Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdOwm2BMY761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "df4b32c1-a8df-433b-a87e-5ccb288a61f4"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/physionet.org/files/mimiciii/1.4/NOTEEVENTS.csv.gz\", low_memory= False)\n",
        "df.head()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ROW_ID</th>\n",
              "      <th>SUBJECT_ID</th>\n",
              "      <th>HADM_ID</th>\n",
              "      <th>CHARTDATE</th>\n",
              "      <th>CHARTTIME</th>\n",
              "      <th>STORETIME</th>\n",
              "      <th>CATEGORY</th>\n",
              "      <th>DESCRIPTION</th>\n",
              "      <th>CGID</th>\n",
              "      <th>ISERROR</th>\n",
              "      <th>TEXT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>174</td>\n",
              "      <td>22532</td>\n",
              "      <td>167853.0</td>\n",
              "      <td>2151-08-04</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Discharge summary</td>\n",
              "      <td>Report</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Admission Date:  [**2151-7-16**]       Dischar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>175</td>\n",
              "      <td>13702</td>\n",
              "      <td>107527.0</td>\n",
              "      <td>2118-06-14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Discharge summary</td>\n",
              "      <td>Report</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Admission Date:  [**2118-6-2**]       Discharg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>176</td>\n",
              "      <td>13702</td>\n",
              "      <td>167118.0</td>\n",
              "      <td>2119-05-25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Discharge summary</td>\n",
              "      <td>Report</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>177</td>\n",
              "      <td>13702</td>\n",
              "      <td>196489.0</td>\n",
              "      <td>2124-08-18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Discharge summary</td>\n",
              "      <td>Report</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Admission Date:  [**2124-7-21**]              ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>178</td>\n",
              "      <td>26880</td>\n",
              "      <td>135453.0</td>\n",
              "      <td>2162-03-25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Discharge summary</td>\n",
              "      <td>Report</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Admission Date:  [**2162-3-3**]              D...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ROW_ID  ...                                               TEXT\n",
              "0     174  ...  Admission Date:  [**2151-7-16**]       Dischar...\n",
              "1     175  ...  Admission Date:  [**2118-6-2**]       Discharg...\n",
              "2     176  ...  Admission Date:  [**2119-5-4**]              D...\n",
              "3     177  ...  Admission Date:  [**2124-7-21**]              ...\n",
              "4     178  ...  Admission Date:  [**2162-3-3**]              D...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zspvpsmwnKr5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "dcbd2a6d-6dae-42c7-ecd1-f92d4c3d10d7"
      },
      "source": [
        "tr_resultdf.head()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>TIME_OBS</th>\n",
              "      <th>TIME_SPAN</th>\n",
              "      <th>imp_N_227465_std</th>\n",
              "      <th>imp_N_225625_avg</th>\n",
              "      <th>imp_N_220179_max</th>\n",
              "      <th>imp_N_227073_avg</th>\n",
              "      <th>imp_N_51237_std</th>\n",
              "      <th>imp_N_220047_TT</th>\n",
              "      <th>imp_N_51222_LT</th>\n",
              "      <th>imp_N_220621_min</th>\n",
              "      <th>imp_N_220181_avg</th>\n",
              "      <th>imp_N_50960_LT</th>\n",
              "      <th>imp_N_51006_avg</th>\n",
              "      <th>imp_N_220180_min</th>\n",
              "      <th>imp_N_223751_LT</th>\n",
              "      <th>imp_N_227442_avg</th>\n",
              "      <th>imp_N_220046_std</th>\n",
              "      <th>imp_N_220181_TT</th>\n",
              "      <th>imp_N_220180_max</th>\n",
              "      <th>imp_N_50960_min</th>\n",
              "      <th>imp_N_51222_avg</th>\n",
              "      <th>imp_N_220046_LT</th>\n",
              "      <th>imp_N_220228_std</th>\n",
              "      <th>imp_N_227442_std</th>\n",
              "      <th>imp_N_50902_TT</th>\n",
              "      <th>imp_N_51265_min</th>\n",
              "      <th>imp_N_223761_std</th>\n",
              "      <th>imp_N_50902_LT</th>\n",
              "      <th>imp_N_50902_std</th>\n",
              "      <th>imp_N_50912_avg</th>\n",
              "      <th>imp_N_51274_max</th>\n",
              "      <th>imp_N_51248_min</th>\n",
              "      <th>imp_N_227467_max</th>\n",
              "      <th>imp_N_227073_TT</th>\n",
              "      <th>imp_N_227465_LT</th>\n",
              "      <th>imp_N_51237_avg</th>\n",
              "      <th>imp_N_223769_std</th>\n",
              "      <th>imp_N_220179_TT</th>\n",
              "      <th>imp_N_223761_min</th>\n",
              "      <th>...</th>\n",
              "      <th>imp_N_223751_avg</th>\n",
              "      <th>imp_N_220645_avg</th>\n",
              "      <th>imp_N_224162_avg</th>\n",
              "      <th>imp_N_50868_LT</th>\n",
              "      <th>imp_N_50902_max</th>\n",
              "      <th>imp_N_50912_std</th>\n",
              "      <th>imp_N_227465_avg</th>\n",
              "      <th>imp_N_220645_max</th>\n",
              "      <th>imp_N_51221_std</th>\n",
              "      <th>imp_N_227467_avg</th>\n",
              "      <th>imp_N_220210_avg</th>\n",
              "      <th>imp_N_51249_avg</th>\n",
              "      <th>imp_N_51250_LT</th>\n",
              "      <th>imp_N_220228_TT</th>\n",
              "      <th>imp_N_51237_min</th>\n",
              "      <th>imp_N_225677_std</th>\n",
              "      <th>imp_N_50983_avg</th>\n",
              "      <th>imp_N_51250_std</th>\n",
              "      <th>imp_N_223751_TT</th>\n",
              "      <th>imp_N_226253_max</th>\n",
              "      <th>imp_N_51265_TT</th>\n",
              "      <th>imp_N_51275_std</th>\n",
              "      <th>imp_N_50970_std</th>\n",
              "      <th>imp_N_51237_max</th>\n",
              "      <th>imp_N_50960_max</th>\n",
              "      <th>imp_N_223770_min</th>\n",
              "      <th>imp_N_220615_std</th>\n",
              "      <th>imp_N_51277_avg</th>\n",
              "      <th>imp_N_220047_std</th>\n",
              "      <th>imp_N_220545_TT</th>\n",
              "      <th>features_imputed</th>\n",
              "      <th>demo_feature</th>\n",
              "      <th>features</th>\n",
              "      <th>DISCH_51881_51851_51884_51853_excl</th>\n",
              "      <th>DISCH_51881_51851_51884_51853_label</th>\n",
              "      <th>label</th>\n",
              "      <th>rawPrediction</th>\n",
              "      <th>probability</th>\n",
              "      <th>prediction</th>\n",
              "      <th>Prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>194216.0</td>\n",
              "      <td>2125-11-06</td>\n",
              "      <td>(2125-11-05 00:00:00, 2125-11-06 00:00:00)</td>\n",
              "      <td>0.130655</td>\n",
              "      <td>8.373489</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.014815</td>\n",
              "      <td>0.041623</td>\n",
              "      <td>0.822296</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>59.181818</td>\n",
              "      <td>0.487134</td>\n",
              "      <td>44.0</td>\n",
              "      <td>31.00000</td>\n",
              "      <td>0.426931</td>\n",
              "      <td>4.750</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.344461e-06</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>2.019275</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.383198</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.35000</td>\n",
              "      <td>0.892992</td>\n",
              "      <td>247.0</td>\n",
              "      <td>0.698570</td>\n",
              "      <td>0.446496</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.40</td>\n",
              "      <td>16.326121</td>\n",
              "      <td>29.8</td>\n",
              "      <td>1.477589</td>\n",
              "      <td>0.047244</td>\n",
              "      <td>0.460002</td>\n",
              "      <td>1.460824</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.163641e-02</td>\n",
              "      <td>96.9</td>\n",
              "      <td>...</td>\n",
              "      <td>160.416362</td>\n",
              "      <td>136.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.977662</td>\n",
              "      <td>103.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>16.227217</td>\n",
              "      <td>136.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.461156</td>\n",
              "      <td>27.708333</td>\n",
              "      <td>33.8</td>\n",
              "      <td>0.330686</td>\n",
              "      <td>0.362813</td>\n",
              "      <td>1.445393</td>\n",
              "      <td>0.089576</td>\n",
              "      <td>136.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.726987</td>\n",
              "      <td>85.0</td>\n",
              "      <td>0.622558</td>\n",
              "      <td>0.876946</td>\n",
              "      <td>0.091198</td>\n",
              "      <td>1.477641</td>\n",
              "      <td>2.093509</td>\n",
              "      <td>90.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>15.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.349556</td>\n",
              "      <td>[0.5050020692768213, 0.9899958614463573, 85.04...</td>\n",
              "      <td>(79.64109589041095, 0.0, 1.0, 0.0, 0.0, 0.0, 0...</td>\n",
              "      <td>(0.5050020692768213, 0.9899958614463573, 85.04...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[1.3963874412694455, -1.3963874412694455]</td>\n",
              "      <td>[0.9422841413262901, 0.057715858673709874]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.057715858673709874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>170247.0</td>\n",
              "      <td>2156-09-17</td>\n",
              "      <td>(2156-09-16 00:00:00, 2156-09-17 00:00:00)</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.600000</td>\n",
              "      <td>183.000000</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.285276</td>\n",
              "      <td>0.540661</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>101.967742</td>\n",
              "      <td>0.011833</td>\n",
              "      <td>67.0</td>\n",
              "      <td>62.00000</td>\n",
              "      <td>0.426931</td>\n",
              "      <td>4.500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.158139e-25</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>1.400000</td>\n",
              "      <td>9.8</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.016272</td>\n",
              "      <td>237.0</td>\n",
              "      <td>0.794090</td>\n",
              "      <td>0.991864</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.70</td>\n",
              "      <td>13.500000</td>\n",
              "      <td>34.3</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>0.185969</td>\n",
              "      <td>0.244142</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.609251e-22</td>\n",
              "      <td>97.4</td>\n",
              "      <td>...</td>\n",
              "      <td>160.416362</td>\n",
              "      <td>143.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.093438</td>\n",
              "      <td>119.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>13.500000</td>\n",
              "      <td>143.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>18.161290</td>\n",
              "      <td>35.2</td>\n",
              "      <td>0.870506</td>\n",
              "      <td>0.925646</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>143.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.726987</td>\n",
              "      <td>85.0</td>\n",
              "      <td>0.692057</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>1.400000</td>\n",
              "      <td>90.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>21.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.878265</td>\n",
              "      <td>[1.0, 4.086259906247097e-24, 115.4482758620689...</td>\n",
              "      <td>(47.8986301369863, 0.0, 1.0, 0.0, 0.0, 0.0, 0....</td>\n",
              "      <td>(1.0, 4.086259906247097e-24, 115.4482758620689...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[0.5703515386897479, -0.5703515386897479]</td>\n",
              "      <td>[0.7578087014694355, 0.24219129853056454]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.24219129853056454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>147237.0</td>\n",
              "      <td>2152-06-13</td>\n",
              "      <td>(2152-06-12 00:00:00, 2152-06-13 00:00:00)</td>\n",
              "      <td>0.130655</td>\n",
              "      <td>8.373489</td>\n",
              "      <td>138.940579</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.014815</td>\n",
              "      <td>0.130766</td>\n",
              "      <td>0.215207</td>\n",
              "      <td>137.000000</td>\n",
              "      <td>74.778262</td>\n",
              "      <td>0.151826</td>\n",
              "      <td>25.0</td>\n",
              "      <td>47.70464</td>\n",
              "      <td>0.426931</td>\n",
              "      <td>3.300</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.071838e-01</td>\n",
              "      <td>78.036967</td>\n",
              "      <td>1.800000</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.303425</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.20000</td>\n",
              "      <td>0.028974</td>\n",
              "      <td>73.0</td>\n",
              "      <td>0.669370</td>\n",
              "      <td>0.985513</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>0.85</td>\n",
              "      <td>16.326121</td>\n",
              "      <td>29.7</td>\n",
              "      <td>1.477589</td>\n",
              "      <td>0.937650</td>\n",
              "      <td>0.460002</td>\n",
              "      <td>1.460824</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.961978e-01</td>\n",
              "      <td>97.6</td>\n",
              "      <td>...</td>\n",
              "      <td>160.416362</td>\n",
              "      <td>146.5</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.537595</td>\n",
              "      <td>115.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>16.227217</td>\n",
              "      <td>148.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.461156</td>\n",
              "      <td>14.781250</td>\n",
              "      <td>32.2</td>\n",
              "      <td>0.601945</td>\n",
              "      <td>0.428681</td>\n",
              "      <td>1.445393</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>146.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.726987</td>\n",
              "      <td>85.0</td>\n",
              "      <td>0.237618</td>\n",
              "      <td>0.876946</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.477641</td>\n",
              "      <td>1.900000</td>\n",
              "      <td>90.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>16.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.605820</td>\n",
              "      <td>[0.9999999931350856, 1.3729828743819694e-08, 1...</td>\n",
              "      <td>(65.7068493150685, 1.0, 0.0, 0.0, 0.0, 0.0, 0....</td>\n",
              "      <td>(0.9999999931350856, 1.3729828743819694e-08, 1...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[0.9471888131157044, -0.9471888131157044]</td>\n",
              "      <td>[0.8692538594617801, 0.13074614053821987]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.13074614053821987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>133334.0</td>\n",
              "      <td>2118-03-08</td>\n",
              "      <td>(2118-03-07 00:00:00, 2118-03-08 00:00:00)</td>\n",
              "      <td>0.130655</td>\n",
              "      <td>7.900000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.014815</td>\n",
              "      <td>0.217266</td>\n",
              "      <td>0.784404</td>\n",
              "      <td>120.724835</td>\n",
              "      <td>77.823529</td>\n",
              "      <td>0.027335</td>\n",
              "      <td>7.0</td>\n",
              "      <td>48.00000</td>\n",
              "      <td>0.341337</td>\n",
              "      <td>4.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.083953e-01</td>\n",
              "      <td>76.000000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>10.8</td>\n",
              "      <td>0.276197</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.492897</td>\n",
              "      <td>346.0</td>\n",
              "      <td>0.530919</td>\n",
              "      <td>0.753552</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.60</td>\n",
              "      <td>16.326121</td>\n",
              "      <td>29.2</td>\n",
              "      <td>1.477589</td>\n",
              "      <td>0.295072</td>\n",
              "      <td>0.460002</td>\n",
              "      <td>1.460824</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.582276e-01</td>\n",
              "      <td>97.7</td>\n",
              "      <td>...</td>\n",
              "      <td>160.000000</td>\n",
              "      <td>138.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.148567</td>\n",
              "      <td>108.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>16.227217</td>\n",
              "      <td>138.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.461156</td>\n",
              "      <td>20.235294</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.128329</td>\n",
              "      <td>0.438981</td>\n",
              "      <td>1.445393</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>138.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.682673</td>\n",
              "      <td>85.0</td>\n",
              "      <td>0.148717</td>\n",
              "      <td>0.876946</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.477641</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>90.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>13.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.566917</td>\n",
              "      <td>[0.10394428699230962, 0.20788857398461924, 80....</td>\n",
              "      <td>(48.45205479452055, 1.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
              "      <td>(0.10394428699230962, 0.20788857398461924, 80....</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[1.4681953423556071, -1.4681953423556071]</td>\n",
              "      <td>[0.94961631849267, 0.05038368150732997]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05038368150732997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>160561.0</td>\n",
              "      <td>2142-08-21</td>\n",
              "      <td>(2142-08-20 00:00:00, 2142-08-21 00:00:00)</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.300000</td>\n",
              "      <td>138.940579</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041623</td>\n",
              "      <td>0.404993</td>\n",
              "      <td>117.000000</td>\n",
              "      <td>74.778262</td>\n",
              "      <td>0.414705</td>\n",
              "      <td>18.0</td>\n",
              "      <td>47.70464</td>\n",
              "      <td>0.426931</td>\n",
              "      <td>3.525</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.071838e-01</td>\n",
              "      <td>78.036967</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>9.3</td>\n",
              "      <td>0.818222</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.34187</td>\n",
              "      <td>0.331550</td>\n",
              "      <td>327.0</td>\n",
              "      <td>0.603922</td>\n",
              "      <td>0.165775</td>\n",
              "      <td>2.061553</td>\n",
              "      <td>0.60</td>\n",
              "      <td>12.400000</td>\n",
              "      <td>29.2</td>\n",
              "      <td>1.100000</td>\n",
              "      <td>0.440415</td>\n",
              "      <td>0.165351</td>\n",
              "      <td>1.100000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.961978e-01</td>\n",
              "      <td>97.8</td>\n",
              "      <td>...</td>\n",
              "      <td>160.416362</td>\n",
              "      <td>130.5</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.222049</td>\n",
              "      <td>104.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>12.400000</td>\n",
              "      <td>133.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.100000</td>\n",
              "      <td>16.041667</td>\n",
              "      <td>34.7</td>\n",
              "      <td>0.128329</td>\n",
              "      <td>0.804919</td>\n",
              "      <td>1.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>130.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.726987</td>\n",
              "      <td>85.0</td>\n",
              "      <td>0.207153</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.100000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>92.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>15.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.675009</td>\n",
              "      <td>[0.9999909972160833, 1.800556783332397e-05, 99...</td>\n",
              "      <td>(62.156164383561645, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>(0.9999909972160833, 1.800556783332397e-05, 99...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[0.978614363702355, -0.978614363702355]</td>\n",
              "      <td>[0.8762327240900406, 0.1237672759099594]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1237672759099594</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 361 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         ID    TIME_OBS  ... prediction                  Prob\n",
              "0  194216.0  2125-11-06  ...        0.0  0.057715858673709874\n",
              "1  170247.0  2156-09-17  ...        0.0   0.24219129853056454\n",
              "2  147237.0  2152-06-13  ...        0.0   0.13074614053821987\n",
              "3  133334.0  2118-03-08  ...        0.0   0.05038368150732997\n",
              "4  160561.0  2142-08-21  ...        0.0    0.1237672759099594\n",
              "\n",
              "[5 rows x 361 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cctUSBwbaRvy"
      },
      "source": [
        "trainingIds = tr_resultdf[\"ID\"]\n",
        "testingIds = te_resultdf[\"ID\"]\n",
        "trainingTimes = tr_resultdf[\"TIME_SPAN\"]\n",
        "testingTimes = te_resultdf[\"TIME_SPAN\"]"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eedX7KUgkHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57e8f4ce-9bc7-4bb7-98ee-013a053199e3"
      },
      "source": [
        "trainingTimes[0][0]"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.datetime(2125, 11, 5, 0, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRjYBIg6n0Wa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "b282c601-0d75-495f-b6bf-c8e546a1f26c"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ROW_ID</th>\n",
              "      <th>SUBJECT_ID</th>\n",
              "      <th>HADM_ID</th>\n",
              "      <th>CHARTDATE</th>\n",
              "      <th>CHARTTIME</th>\n",
              "      <th>STORETIME</th>\n",
              "      <th>CATEGORY</th>\n",
              "      <th>DESCRIPTION</th>\n",
              "      <th>CGID</th>\n",
              "      <th>ISERROR</th>\n",
              "      <th>TEXT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>174</td>\n",
              "      <td>22532</td>\n",
              "      <td>167853.0</td>\n",
              "      <td>2151-08-04</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Discharge summary</td>\n",
              "      <td>Report</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Admission Date:  [**2151-7-16**]       Dischar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>175</td>\n",
              "      <td>13702</td>\n",
              "      <td>107527.0</td>\n",
              "      <td>2118-06-14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Discharge summary</td>\n",
              "      <td>Report</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Admission Date:  [**2118-6-2**]       Discharg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>176</td>\n",
              "      <td>13702</td>\n",
              "      <td>167118.0</td>\n",
              "      <td>2119-05-25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Discharge summary</td>\n",
              "      <td>Report</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>177</td>\n",
              "      <td>13702</td>\n",
              "      <td>196489.0</td>\n",
              "      <td>2124-08-18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Discharge summary</td>\n",
              "      <td>Report</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Admission Date:  [**2124-7-21**]              ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>178</td>\n",
              "      <td>26880</td>\n",
              "      <td>135453.0</td>\n",
              "      <td>2162-03-25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Discharge summary</td>\n",
              "      <td>Report</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Admission Date:  [**2162-3-3**]              D...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ROW_ID  ...                                               TEXT\n",
              "0     174  ...  Admission Date:  [**2151-7-16**]       Dischar...\n",
              "1     175  ...  Admission Date:  [**2118-6-2**]       Discharg...\n",
              "2     176  ...  Admission Date:  [**2119-5-4**]              D...\n",
              "3     177  ...  Admission Date:  [**2124-7-21**]              ...\n",
              "4     178  ...  Admission Date:  [**2162-3-3**]              D...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atwnwTdxhkGz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "e45c91d6-51e9-4204-8544-4784398ed0b7"
      },
      "source": [
        "from datetime import datetime\n",
        "curId = trainingIds[2]\n",
        "print(curId)\n",
        "rowNoteDF = df.loc[df['HADM_ID'] == int(curId)]\n",
        "rowNoteDF = rowNoteDF.reset_index()\n",
        "cur = rowNoteDF[\"CHARTDATE\"][0]\n",
        "print(cur)\n",
        "x = datetime(int(cur[0:4]), int(cur[5:7]), int(cur[-2:]), 1)\n",
        "x\n",
        "print(trainingTimes[2][1])\n",
        "rowNoteDF.head(10)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "147237.0\n",
            "2152-06-13\n",
            "2152-06-13 00:00:00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>ROW_ID</th>\n",
              "      <th>SUBJECT_ID</th>\n",
              "      <th>HADM_ID</th>\n",
              "      <th>CHARTDATE</th>\n",
              "      <th>CHARTTIME</th>\n",
              "      <th>STORETIME</th>\n",
              "      <th>CATEGORY</th>\n",
              "      <th>DESCRIPTION</th>\n",
              "      <th>CGID</th>\n",
              "      <th>ISERROR</th>\n",
              "      <th>TEXT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3328</td>\n",
              "      <td>3181</td>\n",
              "      <td>3969</td>\n",
              "      <td>147237.0</td>\n",
              "      <td>2152-06-13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Discharge summary</td>\n",
              "      <td>Report</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Admission Date:  [**2152-6-4**]              D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>62775</td>\n",
              "      <td>62150</td>\n",
              "      <td>3969</td>\n",
              "      <td>147237.0</td>\n",
              "      <td>2152-06-05</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Echo</td>\n",
              "      <td>Report</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>PATIENT/TEST INFORMATION:\\nIndication: Congest...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>109332</td>\n",
              "      <td>121541</td>\n",
              "      <td>3969</td>\n",
              "      <td>147237.0</td>\n",
              "      <td>2152-06-05</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ECG</td>\n",
              "      <td>Report</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sinus rhythm\\nRight bundle branch block\\nLeft ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>109398</td>\n",
              "      <td>121591</td>\n",
              "      <td>3969</td>\n",
              "      <td>147237.0</td>\n",
              "      <td>2152-06-04</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ECG</td>\n",
              "      <td>Report</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Baseline artifact\\nSinus tachycardia\\nRight bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>109399</td>\n",
              "      <td>121592</td>\n",
              "      <td>3969</td>\n",
              "      <td>147237.0</td>\n",
              "      <td>2152-06-04</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ECG</td>\n",
              "      <td>Report</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sinus tachycardia.  Right bundle-branch block....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>109400</td>\n",
              "      <td>121593</td>\n",
              "      <td>3969</td>\n",
              "      <td>147237.0</td>\n",
              "      <td>2152-05-29</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ECG</td>\n",
              "      <td>Report</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sinus rhythm.  Left atrial abnormality.  Right...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>112469</td>\n",
              "      <td>121540</td>\n",
              "      <td>3969</td>\n",
              "      <td>147237.0</td>\n",
              "      <td>2152-06-12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ECG</td>\n",
              "      <td>Report</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sinus tachycardia\\nRight bundle branch block\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>324346</td>\n",
              "      <td>327813</td>\n",
              "      <td>3969</td>\n",
              "      <td>147237.0</td>\n",
              "      <td>2152-06-04</td>\n",
              "      <td>2152-06-04 09:23:00</td>\n",
              "      <td>2152-06-04 09:23:59</td>\n",
              "      <td>Physician</td>\n",
              "      <td>Physician Attending Admission Note</td>\n",
              "      <td>20066.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Chief Complaint:  DKA\\n   HPI:\\n   65 yo woman...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>324355</td>\n",
              "      <td>327903</td>\n",
              "      <td>3969</td>\n",
              "      <td>147237.0</td>\n",
              "      <td>2152-06-05</td>\n",
              "      <td>2152-06-05 15:14:00</td>\n",
              "      <td>2152-06-05 17:33:45</td>\n",
              "      <td>Nursing</td>\n",
              "      <td>Nursing Progress Note</td>\n",
              "      <td>14411.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pt is a 65 yr old adm [**2152-6-4**] from EW f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>324357</td>\n",
              "      <td>327907</td>\n",
              "      <td>3969</td>\n",
              "      <td>147237.0</td>\n",
              "      <td>2152-06-05</td>\n",
              "      <td>2152-06-05 09:06:00</td>\n",
              "      <td>2152-06-05 18:02:47</td>\n",
              "      <td>Physician</td>\n",
              "      <td>ICU Attending Note</td>\n",
              "      <td>20066.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Clinician:  Attending\\n   Hypotensive overnigh...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index  ROW_ID  ...  ISERROR                                               TEXT\n",
              "0    3328    3181  ...      NaN  Admission Date:  [**2152-6-4**]              D...\n",
              "1   62775   62150  ...      NaN  PATIENT/TEST INFORMATION:\\nIndication: Congest...\n",
              "2  109332  121541  ...      NaN  Sinus rhythm\\nRight bundle branch block\\nLeft ...\n",
              "3  109398  121591  ...      NaN  Baseline artifact\\nSinus tachycardia\\nRight bu...\n",
              "4  109399  121592  ...      NaN  Sinus tachycardia.  Right bundle-branch block....\n",
              "5  109400  121593  ...      NaN  Sinus rhythm.  Left atrial abnormality.  Right...\n",
              "6  112469  121540  ...      NaN  Sinus tachycardia\\nRight bundle branch block\\n...\n",
              "7  324346  327813  ...      NaN  Chief Complaint:  DKA\\n   HPI:\\n   65 yo woman...\n",
              "8  324355  327903  ...      NaN  Pt is a 65 yr old adm [**2152-6-4**] from EW f...\n",
              "9  324357  327907  ...      NaN  Clinician:  Attending\\n   Hypotensive overnigh...\n",
              "\n",
              "[10 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEYk9SiKoVb-"
      },
      "source": [
        "#IMPORTANT PROBLEM - UNDERSTAND WHAT NOTES TO USE USING HADM_ID -> corresponds to each patient visit, figure out what features should like (perhaps n-vector where n is each type of category note)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ7oaf6wacPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8edc28b4-282b-4b02-889a-3338cc404694"
      },
      "source": [
        "trainingText = []\n",
        "for i in range(len(trainingIds)):\n",
        "  curId = trainingIds[i]\n",
        "  rowNoteDF = df.loc[df['ROW_ID'] == int(curId)]\n",
        "  rowNoteDF = rowNoteDF.reset_index()\n",
        "  cur = rowNoteDF[\"CHARTDATE\"][0]\n",
        "  noteDate = datetime(int(cur[0:4]), int(cur[5:7]), int(cur[-2:]), 1)\n",
        "  print(trainingTimes[i][0])\n",
        "  print(noteDate)\n",
        "  print(trainingTimes[i][1])\n",
        "  rowNoteDF.head()\n",
        "  break\n",
        "  if(trainingTimes[i][0] <= noteDate <= trainingTimes[i][1]):\n",
        "    trainingText.append(rowNoteDF[\"TEXT\"][0])\n",
        "  else:\n",
        "    trainingText.append([\"\"])\n",
        "    print(\"strange\")\n",
        "\n",
        "testingText = []\n",
        "for i in range(len(testingIds)):\n",
        "  curId = trainingIds[i]\n",
        "  rowNoteDF = df.loc[df['ROW_ID'] == int(curId)]\n",
        "  rowNoteDF = rowNoteDF.reset_index()\n",
        "  testingText.append(rowNoteDF[\"TEXT\"][0])\n",
        "print(len(trainingText))\n",
        "len(testingText)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2125-11-05 00:00:00\n",
            "2174-07-02 01:00:00\n",
            "2125-11-06 00:00:00\n",
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "321"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2KrhPnYn0__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff16a5d2-116d-4170-e4fe-b6b9b5b8892a"
      },
      "source": [
        "# Tokenizing inputs and saving it in tokenizedInputs\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "tokenizedTraining = []\n",
        "for i in range(len(trainingText)):\n",
        "  tokenizedTraining.append(nltk.word_tokenize(trainingText[i]))\n",
        "tokenizedTesting = []\n",
        "for i in range(len(testingText)):\n",
        "  tokenizedTesting.append(nltk.word_tokenize(testingText[i]))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN1WSJRUlY2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3d12f61-1db0-49de-a164-e1408293816e"
      },
      "source": [
        "import nltk\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import re, string\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "def remove_noise_and_lemmatize(tweet_tokens, stop_words = ()):\n",
        "    cleaned_tokens = []\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mjyXBkNmOZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3422e967-bbe0-4beb-e21c-8cedfa0f8e52"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "cleanedTraining = []  \n",
        "for i in range(len(tokenizedTraining)):\n",
        "  cleanedTraining.append(remove_noise_and_lemmatize(tokenizedTraining[i], stop_words))\n",
        "cleanedTesting = []  \n",
        "for i in range(len(tokenizedTesting)):\n",
        "  cleanedTesting.append(remove_noise_and_lemmatize(tokenizedTesting[i], stop_words))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}